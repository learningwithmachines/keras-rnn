{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Recurrent Neural Network Projects\n",
    "\n",
    "Welcome to the Recurrent Neural Network Project in the Artificial Intelligence Nanodegree! In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'Implementation'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully!\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation TODOs in this notebook\n",
    "\n",
    "This notebook contains two problems, cut into a variety of TODOs.  Make sure to complete each section containing a TODO marker throughout the notebook.  For convenience we provide links to each of these sections below.\n",
    "\n",
    "[TODO #1: Implement a function to window time series](#TODO_1)\n",
    "\n",
    "[TODO #2: Create a simple RNN model using keras to perform regression](#TODO_2)\n",
    "\n",
    "[TODO #3: Finish cleaning a large text corpus](#TODO_3)\n",
    "\n",
    "[TODO #4: Implement a function to window a large text corpus](#TODO_4)\n",
    "\n",
    "[TODO #5: Create a simple RNN model using keras to perform multiclass classification](#TODO_5)\n",
    "\n",
    "[TODO #6: Generate text using a fully trained RNN model and a variety of input sequences](#TODO_6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Perform time series prediction \n",
    "\n",
    "In this project you will perform time series prediction using a Recurrent Neural Network regressor.  In particular you will re-create the figure shown in the notes - where the stock price of Apple was forecasted (or predicted) 7 days in advance.  In completing this exercise you will learn how to construct RNNs using Keras, which will also aid in completing the second project in this notebook.\n",
    "\n",
    "The particular network architecture we will employ for our RNN is known as  [Long Term Short Memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory), which helps significantly avoid technical problems with optimization of RNNs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Getting started\n",
    "\n",
    "First we must load in our time series - a history of around 140 days of Apple's stock price.  Then we need to perform a number of pre-processing steps to prepare it for use with an RNN model.  First off, it is good practice to normalize time series - by normalizing its range.  This helps us avoid serious numerical issues associated how common activation functions (like tanh) transform very large (positive or negative) numbers, as well as helping us to avoid related issues when computing derivatives.\n",
    "\n",
    "Here we normalize the series to lie in the range [0,1] [using this scikit function](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), but it is also commonplace to normalize by a series standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:28.084180Z",
     "start_time": "2018-06-08T15:41:27.903699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### Load in necessary libraries for data input and normalization\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from my_answers import *\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras, lime\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import *\n",
    "from keras.utils.data_utils import get_file\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from my_answers import *\n",
    "# from my_lstmm import *\n",
    "# from lstmm_Udacity_passing import *\n",
    "\n",
    "###tflow_memory\n",
    "def clear_start(clear = False):\n",
    "    K.get_session().close()\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))\n",
    "    if clear:\n",
    "        K.clear_session()\n",
    "\n",
    "### load in and normalize the dataset\n",
    "dataset = np.loadtxt('datasets/normalized_apple_prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a quick look at the (normalized) time series we'll be performing predictions on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:29.705517Z",
     "start_time": "2018-06-08T15:41:29.419732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'normalized series value')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lets take a look at our time series\n",
    "plt.plot(dataset)\n",
    "plt.xlabel('time period')\n",
    "plt.ylabel('normalized series value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Cutting our time series into sequences\n",
    "\n",
    "Remember, our time series is a sequence of numbers that we can represent in general mathematically as \n",
    "\n",
    "$$s_{0},s_{1},s_{2},...,s_{P}$$\n",
    "\n",
    "where $s_{p}$ is the numerical value of the time series at time period $p$ and where $P$ is the total length of the series.  In order to apply our RNN we treat the time series prediction problem as a regression problem, and so need to use a sliding window to construct a set of associated input/output pairs to regress on.  This process is animated in the gif below.\n",
    "\n",
    "<img src=\"images/timeseries_windowing_training.gif\" width=600 height=600/>\n",
    "\n",
    "For example - using a window of size T = 5 (as illustrated in the gif above) we produce a set of input/output pairs like the one shown in the table below\n",
    "\n",
    "$$\\begin{array}{c|c}\n",
    "\\text{Input} & \\text{Output}\\\\\n",
    "\\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\\\\n",
    "\\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\\\\n",
    "\\color{CornflowerBlue}  {\\vdots} & \\color{Goldenrod} {\\vdots}\\\\\n",
    "\\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}}\n",
    "\\end{array}$$\n",
    "\n",
    "Notice here that each input is a sequence (or vector) of length 5 (and in general has length equal to the window size T) while each corresponding output is a scalar value.  Notice also how given a time series of length P and window size T = 5 as shown above, we created P - 5  input/output pairs.  More generally, for a window size T we create P - T such pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time for you to window the input time series as described above!  \n",
    "\n",
    "<a id='TODO_1'></a>\n",
    "\n",
    "**TODO:** Implement the function called **window_transform_series** in my_answers.py so that it runs a sliding window along the input series and creates associated input/output pairs.    Note that this function should input a) the series and b) the window length, and return the input/output subsequences.  Make sure to format returned input/output as generally shown in table above (where window_size = 5), and make sure your returned input is a numpy array.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your function on the list of odd numbers given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:31.809110Z",
     "start_time": "2018-06-08T15:41:31.641639Z"
    }
   },
   "outputs": [],
   "source": [
    "odd_nums = np.array([1,3,5,7,9,11,13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a hard-coded solution for odd_nums.  You can compare its results with what you get from your **window_transform_series** implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:33.167701Z",
     "start_time": "2018-06-08T15:41:33.001260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- the input X will look like ----\n",
      "[[ 1  3]\n",
      " [ 3  5]\n",
      " [ 5  7]\n",
      " [ 7  9]\n",
      " [ 9 11]]\n",
      "--- the associated output y will look like ----\n",
      "[[ 5]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [11]\n",
      " [13]]\n"
     ]
    }
   ],
   "source": [
    "# run a window of size 2 over the odd number sequence and display the results\n",
    "window_size = 2\n",
    "\n",
    "X = []\n",
    "X.append(odd_nums[0:2])\n",
    "X.append(odd_nums[1:3])\n",
    "X.append(odd_nums[2:4])\n",
    "X.append(odd_nums[3:5])\n",
    "X.append(odd_nums[4:6])\n",
    "\n",
    "y = odd_nums[2:]\n",
    "\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "y = np.reshape(y, (len(y),1)) #optional\n",
    "\n",
    "assert(type(X).__name__ == 'ndarray')\n",
    "assert(type(y).__name__ == 'ndarray')\n",
    "assert(X.shape == (5,2))\n",
    "assert(y.shape in [(5,1), (5,)])\n",
    "\n",
    "# print out input/output pairs --> here input = X, corresponding output = y\n",
    "print ('--- the input X will look like ----')\n",
    "print (X)\n",
    "\n",
    "print ('--- the associated output y will look like ----')\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again - you can check that your completed **window_transform_series** function works correctly by trying it on the odd_nums sequence - you should get the above output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:34.485223Z",
     "start_time": "2018-06-08T15:41:34.322773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  3.],\n",
       "        [ 3.,  5.],\n",
       "        [ 5.,  7.],\n",
       "        [ 7.,  9.],\n",
       "        [ 9., 11.]], dtype=float32), array([[ 5.],\n",
       "        [ 7.],\n",
       "        [ 9.],\n",
       "        [11.],\n",
       "        [13.]], dtype=float32))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def window_transform_series_nb(seq, wSize=1, stride=1):\n",
    "    \n",
    "    inputs, outputs  = [],[]\n",
    "    for loc_ in range(0, len(seq) - wSize, stride):\n",
    "        inputs.append(seq[loc_:loc_+wSize])\n",
    "        outputs.append([seq[loc_+wSize]])\n",
    "        \n",
    "    inputs  = np.array(inputs).astype('float32').reshape(-1,wSize)\n",
    "    outputs = np.array(outputs).astype('float32').reshape(-1,1)\n",
    "    return inputs, outputs\n",
    "    \n",
    "window_transform_series(odd_nums, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:35.817750Z",
     "start_time": "2018-06-08T15:41:35.662388Z"
    }
   },
   "outputs": [],
   "source": [
    "### TODO: implement the function window_transform_series in the file my_answers.py\n",
    "from my_answers import window_transform_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function in place apply it to the series in the Python cell below.  We use a window_size = 7 for these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:37.151295Z",
     "start_time": "2018-06-08T15:41:36.982847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- the input X will look like ----\n",
      "[[-0.7006234  -0.8208848  -0.93938303 -0.9471652  -0.68785524 -0.84325904\n",
      "  -0.8053202 ]]\n",
      "--- the associated output y will look like ----\n",
      "[[-0.8205807]]\n"
     ]
    }
   ],
   "source": [
    "# window the data using your windowing function\n",
    "window_size = 7\n",
    "X,y = window_transform_series(seq = dataset,wSize = window_size)\n",
    "\n",
    "# print out input/output pairs --> here input = X, corresponding output = y\n",
    "print ('--- the input X will look like ----')\n",
    "print (X[:1])\n",
    "\n",
    "print ('--- the associated output y will look like ----')\n",
    "print (y[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3  Splitting into training and testing sets\n",
    "\n",
    "In order to perform proper testing on our dataset we will lop off the last 1/3 of it for validation (or testing).  This is that once we train our model we have something to test it on (like any regression problem!).  This splitting into training/testing sets is done in the cell below.\n",
    "\n",
    "Note how here we are **not** splitting the dataset *randomly* as one typically would do when validating a regression model.  This is because our input/output pairs *are related temporally*.   We don't want to validate our model by training on a random subset of the series and then testing on another random subset, as this simulates the scenario that we receive new points *within the timeframe of our training set*.  \n",
    "\n",
    "We want to train on one solid chunk of the series (in our case, the first full 2/3 of it), and validate on a later chunk (the last 1/3) as this simulates how we would predict *future* values of a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:38.555413Z",
     "start_time": "2018-06-08T15:41:38.385957Z"
    }
   },
   "outputs": [],
   "source": [
    "# split our dataset into training / testing sets\n",
    "train_test_split = int(np.ceil(2*len(y)/float(3)))   # set the split point\n",
    "\n",
    "# partition the training set\n",
    "X_train = X[:train_test_split,:]\n",
    "y_train = y[:train_test_split]\n",
    "\n",
    "# keep the last chunk for testing\n",
    "X_test = X[train_test_split:,:]\n",
    "y_test = y[train_test_split:]\n",
    "\n",
    "# NOTE: to use keras's RNN LSTM module our input must be reshaped to [samples, window size, stepsize] \n",
    "X_train = np.asarray(np.reshape(X_train, (X_train.shape[0], window_size, 1)))\n",
    "X_test = np.asarray(np.reshape(X_test, (X_test.shape[0], window_size, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:41:39.918031Z",
     "start_time": "2018-06-08T15:41:39.758607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88, 7, 1), (88, 1))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_2'></a>\n",
    "\n",
    "## 1.4  Build and run an RNN regression model\n",
    "\n",
    "Having created input/output pairs out of our time series and cut this into training/testing sets, we can now begin setting up our RNN.  We use Keras to quickly build a two hidden layer RNN of the following specifications\n",
    "\n",
    "- layer 1 uses an LSTM module with 5 hidden units (note here the input_shape = (window_size,1))\n",
    "- layer 2 uses a fully connected module with one unit\n",
    "- the 'mean_squared_error' loss should be used (remember: we are performing regression here)\n",
    "\n",
    "This can be constructed using just a few lines - see e.g., the [general Keras documentation](https://keras.io/getting-started/sequential-model-guide/) and the [LSTM documentation in particular](https://keras.io/layers/recurrent/) for examples of how to quickly use Keras to build neural network models.  Make sure you are initializing your optimizer given the [keras-recommended approach for RNNs](https://keras.io/optimizers/) \n",
    "\n",
    "(given in the cell below).  (remember to copy your completed function into the script *my_answers.py* function titled *build_part1_RNN* before submitting your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:42:47.759600Z",
     "start_time": "2018-06-08T15:42:47.272306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 5)                 140       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 146\n",
      "Trainable params: 146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: create required RNN model\n",
    "# import keras network libraries - # imported in previous class declaration\n",
    "\n",
    "# given - fix random seed - so we can all reproduce the same results on our default time series\n",
    "np.random.seed(0)\n",
    "\n",
    "# TODO: implement build_part1_RNN in my_answers.py\n",
    "from my_answers import build_part1_RNN\n",
    "model  = build_part1_RNN(step_size=1, windowsize=7)\n",
    "\n",
    "# build model using keras documentation recommended optimizer initialization\n",
    "optimizer = rmsprop(lr=0.1, rho=0.9, epsilon=1e-08, decay=0.1)\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your model built you can now fit the model by activating the cell below!  Note: the number of epochs (np_epochs) and batch_size are preset (so we can all produce the same results).  You can choose to toggle the verbose parameter - which gives you regular updates on the progress of the algorithm - on and off by setting it to 1 or 0 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:12.687166Z",
     "start_time": "2018-06-08T15:42:49.115707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: loss improved from inf to 0.14398, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.14398\n",
      "\n",
      "Epoch 00003: loss improved from 0.14398 to 0.05305, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 0.05305 to 0.04910, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 0.04910 to 0.04619, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 0.04619 to 0.04392, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 0.04392 to 0.04210, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00008: loss improved from 0.04210 to 0.04061, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00009: loss improved from 0.04061 to 0.03936, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 0.03936 to 0.03829, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 0.03829 to 0.03737, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 0.03737 to 0.03655, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 0.03655 to 0.03583, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 0.03583 to 0.03518, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 0.03518 to 0.03459, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 0.03459 to 0.03405, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00017: loss improved from 0.03405 to 0.03355, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 0.03355 to 0.03308, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 0.03308 to 0.03265, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 0.03265 to 0.03223, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 0.03223 to 0.03184, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 0.03184 to 0.03146, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 0.03146 to 0.03110, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 0.03110 to 0.03076, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 0.03076 to 0.03042, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 0.03042 to 0.03009, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 0.03009 to 0.02977, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 0.02977 to 0.02945, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 0.02945 to 0.02914, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 0.02914 to 0.02884, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 0.02884 to 0.02854, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 0.02854 to 0.02824, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 0.02824 to 0.02795, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 0.02795 to 0.02766, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 0.02766 to 0.02737, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 0.02737 to 0.02709, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 0.02709 to 0.02681, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 0.02681 to 0.02653, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 0.02653 to 0.02626, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.02626 to 0.02599, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.02599 to 0.02572, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.02572 to 0.02546, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.02546 to 0.02520, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.02520 to 0.02494, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.02494 to 0.02469, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.02469 to 0.02445, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.02445 to 0.02421, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.02421 to 0.02398, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.02398 to 0.02375, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.02375 to 0.02353, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.02353 to 0.02332, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.02332 to 0.02311, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.02311 to 0.02291, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.02291 to 0.02272, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.02272 to 0.02253, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.02253 to 0.02235, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.02235 to 0.02218, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00058: loss improved from 0.02218 to 0.02201, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00059: loss improved from 0.02201 to 0.02186, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.02186 to 0.02170, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00061: loss improved from 0.02170 to 0.02156, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00062: loss improved from 0.02156 to 0.02141, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00063: loss improved from 0.02141 to 0.02128, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.02128 to 0.02115, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00065: loss improved from 0.02115 to 0.02102, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00066: loss improved from 0.02102 to 0.02092, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.02092\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.02092\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.02092\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.02092\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.02092\n",
      "\n",
      "Epoch 00072: loss improved from 0.02092 to 0.02087, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.02087 to 0.02053, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.02053 to 0.02032, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00075: loss improved from 0.02032 to 0.02018, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00076: loss improved from 0.02018 to 0.02007, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.02007 to 0.01998, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.01998 to 0.01990, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.01990 to 0.01984, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.01984 to 0.01978, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.01978 to 0.01975, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.01975\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.01975\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.01975\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.01975\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.01975\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.01975\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.01975\n",
      "\n",
      "Epoch 00089: loss improved from 0.01975 to 0.01963, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00090: loss improved from 0.01963 to 0.01938, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00091: loss improved from 0.01938 to 0.01925, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00092: loss improved from 0.01925 to 0.01912, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00093: loss improved from 0.01912 to 0.01904, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00094: loss improved from 0.01904 to 0.01897, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00095: loss improved from 0.01897 to 0.01893, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00096: loss improved from 0.01893 to 0.01889, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00097: loss improved from 0.01889 to 0.01888, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00098: loss improved from 0.01888 to 0.01888, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.01888\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.01888\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.01888\n",
      "\n",
      "Epoch 00102: loss did not improve from 0.01888\n",
      "\n",
      "Epoch 00103: loss improved from 0.01888 to 0.01883, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00104: loss improved from 0.01883 to 0.01870, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00105: loss improved from 0.01870 to 0.01860, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00106: loss improved from 0.01860 to 0.01849, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00107: loss improved from 0.01849 to 0.01842, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00108: loss improved from 0.01842 to 0.01836, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00109: loss improved from 0.01836 to 0.01834, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.01834\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.01834\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.01834\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.01834\n",
      "\n",
      "Epoch 00114: loss improved from 0.01834 to 0.01824, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00115: loss improved from 0.01824 to 0.01812, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00116: loss improved from 0.01812 to 0.01803, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00117: loss improved from 0.01803 to 0.01796, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00118: loss improved from 0.01796 to 0.01792, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00119: loss improved from 0.01792 to 0.01790, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.01790\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.01790\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.01790\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.01790\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.01790\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.01790\n",
      "\n",
      "Epoch 00126: loss did not improve from 0.01790\n",
      "\n",
      "Epoch 00127: loss improved from 0.01790 to 0.01778, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00128: loss improved from 0.01778 to 0.01767, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00129: loss improved from 0.01767 to 0.01761, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00130: loss improved from 0.01761 to 0.01757, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00131: loss improved from 0.01757 to 0.01757, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00132: loss did not improve from 0.01757\n",
      "\n",
      "Epoch 00133: loss did not improve from 0.01757\n",
      "\n",
      "Epoch 00134: loss did not improve from 0.01757\n",
      "\n",
      "Epoch 00135: loss did not improve from 0.01757\n",
      "\n",
      "Epoch 00136: loss did not improve from 0.01757\n",
      "\n",
      "Epoch 00137: loss did not improve from 0.01757\n",
      "\n",
      "Epoch 00138: loss improved from 0.01757 to 0.01751, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00139: loss improved from 0.01751 to 0.01748, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00140: loss improved from 0.01748 to 0.01747, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00141: loss did not improve from 0.01747\n",
      "\n",
      "Epoch 00142: loss did not improve from 0.01747\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.01747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00144: loss improved from 0.01747 to 0.01746, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00145: loss improved from 0.01746 to 0.01744, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00146: loss improved from 0.01744 to 0.01740, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00147: loss improved from 0.01740 to 0.01738, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00148: loss improved from 0.01738 to 0.01737, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.01737\n",
      "\n",
      "Epoch 00150: loss did not improve from 0.01737\n",
      "\n",
      "Epoch 00151: loss did not improve from 0.01737\n",
      "\n",
      "Epoch 00152: loss improved from 0.01737 to 0.01735, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00153: loss improved from 0.01735 to 0.01729, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00154: loss improved from 0.01729 to 0.01722, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00155: loss improved from 0.01722 to 0.01718, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00156: loss improved from 0.01718 to 0.01714, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00157: loss improved from 0.01714 to 0.01713, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00158: loss improved from 0.01713 to 0.01713, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00159: loss did not improve from 0.01713\n",
      "\n",
      "Epoch 00160: loss did not improve from 0.01713\n",
      "\n",
      "Epoch 00161: loss did not improve from 0.01713\n",
      "\n",
      "Epoch 00162: loss did not improve from 0.01713\n",
      "\n",
      "Epoch 00163: loss did not improve from 0.01713\n",
      "\n",
      "Epoch 00164: loss improved from 0.01713 to 0.01713, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00165: loss improved from 0.01713 to 0.01709, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00166: loss improved from 0.01709 to 0.01706, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00167: loss improved from 0.01706 to 0.01704, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00168: loss improved from 0.01704 to 0.01704, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00169: loss did not improve from 0.01704\n",
      "\n",
      "Epoch 00170: loss improved from 0.01704 to 0.01702, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00171: loss improved from 0.01702 to 0.01698, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00172: loss improved from 0.01698 to 0.01694, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00173: loss improved from 0.01694 to 0.01691, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00174: loss improved from 0.01691 to 0.01690, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00175: loss did not improve from 0.01690\n",
      "\n",
      "Epoch 00176: loss did not improve from 0.01690\n",
      "\n",
      "Epoch 00177: loss did not improve from 0.01690\n",
      "\n",
      "Epoch 00178: loss did not improve from 0.01690\n",
      "\n",
      "Epoch 00179: loss did not improve from 0.01690\n",
      "\n",
      "Epoch 00180: loss did not improve from 0.01690\n",
      "\n",
      "Epoch 00181: loss improved from 0.01690 to 0.01688, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00182: loss improved from 0.01688 to 0.01684, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00183: loss improved from 0.01684 to 0.01682, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00184: loss improved from 0.01682 to 0.01681, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00185: loss improved from 0.01681 to 0.01681, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00186: loss improved from 0.01681 to 0.01680, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00187: loss improved from 0.01680 to 0.01679, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00188: loss improved from 0.01679 to 0.01676, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00189: loss improved from 0.01676 to 0.01675, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00190: loss improved from 0.01675 to 0.01673, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00191: loss improved from 0.01673 to 0.01673, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00192: loss improved from 0.01673 to 0.01673, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00193: loss did not improve from 0.01673\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.01673\n",
      "\n",
      "Epoch 00195: loss did not improve from 0.01673\n",
      "\n",
      "Epoch 00196: loss improved from 0.01673 to 0.01671, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00197: loss improved from 0.01671 to 0.01669, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00198: loss improved from 0.01669 to 0.01666, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00199: loss improved from 0.01666 to 0.01665, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00200: loss improved from 0.01665 to 0.01663, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00201: loss improved from 0.01663 to 0.01663, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00202: loss improved from 0.01663 to 0.01662, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00203: loss improved from 0.01662 to 0.01661, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00204: loss improved from 0.01661 to 0.01660, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00205: loss improved from 0.01660 to 0.01659, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00206: loss improved from 0.01659 to 0.01657, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00207: loss improved from 0.01657 to 0.01657, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00208: loss improved from 0.01657 to 0.01656, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00209: loss did not improve from 0.01656\n",
      "\n",
      "Epoch 00210: loss improved from 0.01656 to 0.01656, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00211: loss improved from 0.01656 to 0.01655, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00212: loss improved from 0.01655 to 0.01653, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00213: loss improved from 0.01653 to 0.01652, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00214: loss improved from 0.01652 to 0.01650, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00215: loss improved from 0.01650 to 0.01650, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00216: loss improved from 0.01650 to 0.01648, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00217: loss improved from 0.01648 to 0.01648, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00218: loss improved from 0.01648 to 0.01647, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00219: loss improved from 0.01647 to 0.01646, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00220: loss improved from 0.01646 to 0.01645, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00221: loss improved from 0.01645 to 0.01644, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00222: loss improved from 0.01644 to 0.01643, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00223: loss improved from 0.01643 to 0.01642, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00224: loss improved from 0.01642 to 0.01642, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00225: loss improved from 0.01642 to 0.01641, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00226: loss improved from 0.01641 to 0.01640, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00227: loss improved from 0.01640 to 0.01640, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00228: loss improved from 0.01640 to 0.01638, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00229: loss improved from 0.01638 to 0.01638, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00230: loss improved from 0.01638 to 0.01636, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00231: loss improved from 0.01636 to 0.01636, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00232: loss improved from 0.01636 to 0.01635, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00233: loss improved from 0.01635 to 0.01634, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00234: loss improved from 0.01634 to 0.01633, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00235: loss improved from 0.01633 to 0.01632, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00236: loss improved from 0.01632 to 0.01631, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00237: loss improved from 0.01631 to 0.01631, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00238: loss improved from 0.01631 to 0.01630, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00239: loss improved from 0.01630 to 0.01630, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00240: loss improved from 0.01630 to 0.01629, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00241: loss improved from 0.01629 to 0.01628, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00242: loss improved from 0.01628 to 0.01627, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00243: loss improved from 0.01627 to 0.01627, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00244: loss improved from 0.01627 to 0.01626, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00245: loss improved from 0.01626 to 0.01625, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00246: loss improved from 0.01625 to 0.01624, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00247: loss improved from 0.01624 to 0.01623, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00248: loss improved from 0.01623 to 0.01622, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00249: loss improved from 0.01622 to 0.01622, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00250: loss improved from 0.01622 to 0.01621, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00251: loss improved from 0.01621 to 0.01620, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00252: loss improved from 0.01620 to 0.01620, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00253: loss improved from 0.01620 to 0.01619, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00254: loss improved from 0.01619 to 0.01618, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00255: loss improved from 0.01618 to 0.01618, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00256: loss improved from 0.01618 to 0.01617, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00257: loss improved from 0.01617 to 0.01617, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00258: loss improved from 0.01617 to 0.01616, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00259: loss improved from 0.01616 to 0.01615, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00260: loss improved from 0.01615 to 0.01614, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00261: loss improved from 0.01614 to 0.01614, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00262: loss improved from 0.01614 to 0.01613, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00263: loss improved from 0.01613 to 0.01612, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00264: loss improved from 0.01612 to 0.01612, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00265: loss improved from 0.01612 to 0.01611, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00266: loss improved from 0.01611 to 0.01610, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00267: loss improved from 0.01610 to 0.01610, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00268: loss improved from 0.01610 to 0.01609, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00269: loss improved from 0.01609 to 0.01609, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00270: loss improved from 0.01609 to 0.01608, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00271: loss improved from 0.01608 to 0.01608, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00272: loss improved from 0.01608 to 0.01607, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00273: loss improved from 0.01607 to 0.01606, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00274: loss improved from 0.01606 to 0.01605, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00275: loss improved from 0.01605 to 0.01605, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00276: loss improved from 0.01605 to 0.01604, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00277: loss improved from 0.01604 to 0.01604, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00278: loss improved from 0.01604 to 0.01603, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00279: loss improved from 0.01603 to 0.01602, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00280: loss improved from 0.01602 to 0.01602, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00281: loss improved from 0.01602 to 0.01601, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00282: loss improved from 0.01601 to 0.01601, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00283: loss improved from 0.01601 to 0.01600, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00284: loss improved from 0.01600 to 0.01600, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00285: loss improved from 0.01600 to 0.01599, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00286: loss improved from 0.01599 to 0.01598, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00287: loss improved from 0.01598 to 0.01598, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00288: loss improved from 0.01598 to 0.01597, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00289: loss improved from 0.01597 to 0.01597, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00290: loss improved from 0.01597 to 0.01596, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00291: loss improved from 0.01596 to 0.01596, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00292: loss improved from 0.01596 to 0.01595, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00293: loss improved from 0.01595 to 0.01595, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00294: loss improved from 0.01595 to 0.01594, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00295: loss improved from 0.01594 to 0.01593, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00296: loss improved from 0.01593 to 0.01593, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00297: loss improved from 0.01593 to 0.01592, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00298: loss improved from 0.01592 to 0.01592, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00299: loss improved from 0.01592 to 0.01591, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00300: loss improved from 0.01591 to 0.01591, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00301: loss improved from 0.01591 to 0.01590, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00302: loss improved from 0.01590 to 0.01590, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00303: loss improved from 0.01590 to 0.01589, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00304: loss improved from 0.01589 to 0.01589, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00305: loss improved from 0.01589 to 0.01588, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00306: loss improved from 0.01588 to 0.01588, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00307: loss improved from 0.01588 to 0.01587, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00308: loss improved from 0.01587 to 0.01587, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00309: loss improved from 0.01587 to 0.01586, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00310: loss improved from 0.01586 to 0.01586, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00311: loss improved from 0.01586 to 0.01585, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00312: loss improved from 0.01585 to 0.01585, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00313: loss improved from 0.01585 to 0.01584, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00314: loss improved from 0.01584 to 0.01584, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00315: loss improved from 0.01584 to 0.01583, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00316: loss improved from 0.01583 to 0.01583, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00317: loss improved from 0.01583 to 0.01582, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00318: loss improved from 0.01582 to 0.01582, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00319: loss improved from 0.01582 to 0.01581, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00320: loss improved from 0.01581 to 0.01581, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00321: loss improved from 0.01581 to 0.01580, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00322: loss improved from 0.01580 to 0.01580, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00323: loss improved from 0.01580 to 0.01579, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00324: loss improved from 0.01579 to 0.01579, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00325: loss improved from 0.01579 to 0.01578, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00326: loss improved from 0.01578 to 0.01578, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00327: loss improved from 0.01578 to 0.01577, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00328: loss improved from 0.01577 to 0.01577, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00329: loss improved from 0.01577 to 0.01577, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00330: loss improved from 0.01577 to 0.01576, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00331: loss improved from 0.01576 to 0.01575, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00332: loss improved from 0.01575 to 0.01575, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00333: loss improved from 0.01575 to 0.01574, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00334: loss improved from 0.01574 to 0.01574, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00335: loss improved from 0.01574 to 0.01574, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00336: loss improved from 0.01574 to 0.01573, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00337: loss improved from 0.01573 to 0.01573, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00338: loss improved from 0.01573 to 0.01572, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00339: loss improved from 0.01572 to 0.01572, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00340: loss improved from 0.01572 to 0.01571, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00341: loss improved from 0.01571 to 0.01571, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00342: loss improved from 0.01571 to 0.01570, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00343: loss improved from 0.01570 to 0.01570, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00344: loss improved from 0.01570 to 0.01570, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00345: loss improved from 0.01570 to 0.01569, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00346: loss improved from 0.01569 to 0.01568, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00347: loss improved from 0.01568 to 0.01568, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00348: loss improved from 0.01568 to 0.01567, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00349: loss improved from 0.01567 to 0.01567, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00350: loss improved from 0.01567 to 0.01567, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00351: loss improved from 0.01567 to 0.01566, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00352: loss improved from 0.01566 to 0.01566, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00353: loss improved from 0.01566 to 0.01565, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00354: loss improved from 0.01565 to 0.01565, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00355: loss improved from 0.01565 to 0.01564, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00356: loss improved from 0.01564 to 0.01564, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00357: loss improved from 0.01564 to 0.01563, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00358: loss improved from 0.01563 to 0.01563, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00359: loss improved from 0.01563 to 0.01563, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00360: loss improved from 0.01563 to 0.01562, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00361: loss improved from 0.01562 to 0.01562, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00362: loss improved from 0.01562 to 0.01561, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00363: loss improved from 0.01561 to 0.01561, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00364: loss improved from 0.01561 to 0.01560, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00365: loss improved from 0.01560 to 0.01560, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00366: loss improved from 0.01560 to 0.01559, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00367: loss improved from 0.01559 to 0.01559, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00368: loss improved from 0.01559 to 0.01559, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00369: loss improved from 0.01559 to 0.01558, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00370: loss improved from 0.01558 to 0.01557, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00371: loss improved from 0.01557 to 0.01557, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00372: loss improved from 0.01557 to 0.01556, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00373: loss improved from 0.01556 to 0.01556, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00374: loss improved from 0.01556 to 0.01556, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00375: loss improved from 0.01556 to 0.01556, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00376: loss improved from 0.01556 to 0.01555, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00377: loss improved from 0.01555 to 0.01555, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00378: loss improved from 0.01555 to 0.01554, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00379: loss improved from 0.01554 to 0.01554, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00380: loss improved from 0.01554 to 0.01553, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00381: loss improved from 0.01553 to 0.01553, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00382: loss improved from 0.01553 to 0.01553, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00383: loss improved from 0.01553 to 0.01552, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00384: loss improved from 0.01552 to 0.01552, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00385: loss improved from 0.01552 to 0.01552, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00386: loss improved from 0.01552 to 0.01551, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00387: loss improved from 0.01551 to 0.01550, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00388: loss improved from 0.01550 to 0.01550, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00389: loss improved from 0.01550 to 0.01550, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00390: loss improved from 0.01550 to 0.01549, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00391: loss improved from 0.01549 to 0.01549, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00392: loss improved from 0.01549 to 0.01549, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00393: loss improved from 0.01549 to 0.01548, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00394: loss improved from 0.01548 to 0.01548, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00395: loss improved from 0.01548 to 0.01547, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00396: loss improved from 0.01547 to 0.01547, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00397: loss improved from 0.01547 to 0.01546, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00398: loss improved from 0.01546 to 0.01546, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00399: loss improved from 0.01546 to 0.01546, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00400: loss improved from 0.01546 to 0.01546, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00401: loss improved from 0.01546 to 0.01545, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00402: loss improved from 0.01545 to 0.01545, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00403: loss improved from 0.01545 to 0.01544, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00404: loss improved from 0.01544 to 0.01543, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00405: loss improved from 0.01543 to 0.01543, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00406: loss improved from 0.01543 to 0.01543, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00407: loss improved from 0.01543 to 0.01543, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00408: loss improved from 0.01543 to 0.01543, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00409: loss improved from 0.01543 to 0.01542, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00410: loss improved from 0.01542 to 0.01542, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00411: loss improved from 0.01542 to 0.01541, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00412: loss improved from 0.01541 to 0.01540, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00413: loss improved from 0.01540 to 0.01540, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00414: loss improved from 0.01540 to 0.01540, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00415: loss improved from 0.01540 to 0.01540, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00416: loss improved from 0.01540 to 0.01540, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00417: loss improved from 0.01540 to 0.01539, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00418: loss improved from 0.01539 to 0.01539, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00419: loss improved from 0.01539 to 0.01538, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00420: loss improved from 0.01538 to 0.01537, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00421: loss improved from 0.01537 to 0.01537, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00422: loss improved from 0.01537 to 0.01537, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00423: loss improved from 0.01537 to 0.01537, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00424: loss improved from 0.01537 to 0.01536, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00425: loss improved from 0.01536 to 0.01536, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00426: loss improved from 0.01536 to 0.01536, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00427: loss improved from 0.01536 to 0.01535, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00428: loss improved from 0.01535 to 0.01534, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00429: loss improved from 0.01534 to 0.01534, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00430: loss improved from 0.01534 to 0.01533, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00431: loss improved from 0.01533 to 0.01533, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00432: loss improved from 0.01533 to 0.01533, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00433: loss did not improve from 0.01533\n",
      "\n",
      "Epoch 00434: loss improved from 0.01533 to 0.01533, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00435: loss improved from 0.01533 to 0.01533, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00436: loss improved from 0.01533 to 0.01532, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00437: loss improved from 0.01532 to 0.01531, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00438: loss improved from 0.01531 to 0.01531, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00439: loss improved from 0.01531 to 0.01530, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00440: loss improved from 0.01530 to 0.01530, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00441: loss did not improve from 0.01530\n",
      "\n",
      "Epoch 00442: loss improved from 0.01530 to 0.01530, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00443: loss improved from 0.01530 to 0.01530, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00444: loss improved from 0.01530 to 0.01529, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00445: loss improved from 0.01529 to 0.01528, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00446: loss improved from 0.01528 to 0.01528, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00447: loss improved from 0.01528 to 0.01527, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00448: loss improved from 0.01527 to 0.01527, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00449: loss improved from 0.01527 to 0.01527, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00450: loss improved from 0.01527 to 0.01527, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00451: loss improved from 0.01527 to 0.01527, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00452: loss improved from 0.01527 to 0.01526, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00453: loss improved from 0.01526 to 0.01526, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00454: loss improved from 0.01526 to 0.01525, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00455: loss improved from 0.01525 to 0.01524, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00456: loss improved from 0.01524 to 0.01524, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00457: loss improved from 0.01524 to 0.01524, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00458: loss improved from 0.01524 to 0.01524, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00459: loss did not improve from 0.01524\n",
      "\n",
      "Epoch 00460: loss improved from 0.01524 to 0.01524, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00461: loss improved from 0.01524 to 0.01523, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00462: loss improved from 0.01523 to 0.01523, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00463: loss improved from 0.01523 to 0.01522, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00464: loss improved from 0.01522 to 0.01521, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00465: loss improved from 0.01521 to 0.01521, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00466: loss improved from 0.01521 to 0.01521, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00467: loss improved from 0.01521 to 0.01521, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00468: loss improved from 0.01521 to 0.01521, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00469: loss improved from 0.01521 to 0.01520, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00470: loss improved from 0.01520 to 0.01520, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00471: loss improved from 0.01520 to 0.01519, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00472: loss improved from 0.01519 to 0.01519, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00473: loss improved from 0.01519 to 0.01518, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00474: loss improved from 0.01518 to 0.01518, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00475: loss improved from 0.01518 to 0.01518, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00476: loss did not improve from 0.01518\n",
      "\n",
      "Epoch 00477: loss did not improve from 0.01518\n",
      "\n",
      "Epoch 00478: loss improved from 0.01518 to 0.01517, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00479: loss improved from 0.01517 to 0.01517, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00480: loss improved from 0.01517 to 0.01516, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00481: loss improved from 0.01516 to 0.01514, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00482: loss improved from 0.01514 to 0.01512, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00483: loss improved from 0.01512 to 0.01511, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00484: loss improved from 0.01511 to 0.01510, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00485: loss improved from 0.01510 to 0.01510, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00486: loss improved from 0.01510 to 0.01509, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00487: loss improved from 0.01509 to 0.01507, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00488: loss improved from 0.01507 to 0.01506, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00489: loss improved from 0.01506 to 0.01505, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00490: loss improved from 0.01505 to 0.01504, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00491: loss improved from 0.01504 to 0.01502, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00492: loss improved from 0.01502 to 0.01501, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00493: loss improved from 0.01501 to 0.01501, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00494: loss improved from 0.01501 to 0.01501, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00495: loss improved from 0.01501 to 0.01501, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00496: loss improved from 0.01501 to 0.01500, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00497: loss improved from 0.01500 to 0.01500, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00498: loss improved from 0.01500 to 0.01500, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00499: loss improved from 0.01500 to 0.01499, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00500: loss improved from 0.01499 to 0.01499, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00501: loss improved from 0.01499 to 0.01498, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00502: loss improved from 0.01498 to 0.01498, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00503: loss improved from 0.01498 to 0.01497, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00504: loss did not improve from 0.01497\n",
      "\n",
      "Epoch 00505: loss improved from 0.01497 to 0.01497, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00506: loss improved from 0.01497 to 0.01496, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00507: loss improved from 0.01496 to 0.01495, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00508: loss improved from 0.01495 to 0.01495, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00509: loss improved from 0.01495 to 0.01495, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00510: loss did not improve from 0.01495\n",
      "\n",
      "Epoch 00511: loss did not improve from 0.01495\n",
      "\n",
      "Epoch 00512: loss did not improve from 0.01495\n",
      "\n",
      "Epoch 00513: loss did not improve from 0.01495\n",
      "\n",
      "Epoch 00514: loss did not improve from 0.01495\n",
      "\n",
      "Epoch 00515: loss improved from 0.01495 to 0.01494, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00516: loss improved from 0.01494 to 0.01493, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00517: loss improved from 0.01493 to 0.01493, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00518: loss improved from 0.01493 to 0.01493, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00519: loss improved from 0.01493 to 0.01492, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00520: loss improved from 0.01492 to 0.01492, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00521: loss did not improve from 0.01492\n",
      "\n",
      "Epoch 00522: loss improved from 0.01492 to 0.01492, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00523: loss improved from 0.01492 to 0.01492, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00524: loss did not improve from 0.01492\n",
      "\n",
      "Epoch 00525: loss improved from 0.01492 to 0.01491, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00526: loss improved from 0.01491 to 0.01490, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00527: loss improved from 0.01490 to 0.01490, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00528: loss improved from 0.01490 to 0.01490, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00529: loss did not improve from 0.01490\n",
      "\n",
      "Epoch 00530: loss did not improve from 0.01490\n",
      "\n",
      "Epoch 00531: loss did not improve from 0.01490\n",
      "\n",
      "Epoch 00532: loss improved from 0.01490 to 0.01490, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00533: loss improved from 0.01490 to 0.01489, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00534: loss improved from 0.01489 to 0.01488, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00535: loss improved from 0.01488 to 0.01488, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00536: loss improved from 0.01488 to 0.01488, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00537: loss did not improve from 0.01488\n",
      "\n",
      "Epoch 00538: loss did not improve from 0.01488\n",
      "\n",
      "Epoch 00539: loss did not improve from 0.01488\n",
      "\n",
      "Epoch 00540: loss did not improve from 0.01488\n",
      "\n",
      "Epoch 00541: loss improved from 0.01488 to 0.01487, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00542: loss improved from 0.01487 to 0.01486, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00543: loss improved from 0.01486 to 0.01486, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00544: loss improved from 0.01486 to 0.01485, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00545: loss improved from 0.01485 to 0.01485, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00546: loss did not improve from 0.01485\n",
      "\n",
      "Epoch 00547: loss did not improve from 0.01485\n",
      "\n",
      "Epoch 00548: loss improved from 0.01485 to 0.01485, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00549: loss did not improve from 0.01485\n",
      "\n",
      "Epoch 00550: loss improved from 0.01485 to 0.01485, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00551: loss improved from 0.01485 to 0.01484, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00552: loss improved from 0.01484 to 0.01484, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00553: loss improved from 0.01484 to 0.01484, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00554: loss improved from 0.01484 to 0.01483, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00555: loss did not improve from 0.01483\n",
      "\n",
      "Epoch 00556: loss improved from 0.01483 to 0.01482, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00557: loss improved from 0.01482 to 0.01482, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00558: loss improved from 0.01482 to 0.01481, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00559: loss did not improve from 0.01481\n",
      "\n",
      "Epoch 00560: loss improved from 0.01481 to 0.01481, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00561: loss did not improve from 0.01481\n",
      "\n",
      "Epoch 00562: loss improved from 0.01481 to 0.01481, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00563: loss did not improve from 0.01481\n",
      "\n",
      "Epoch 00564: loss did not improve from 0.01481\n",
      "\n",
      "Epoch 00565: loss did not improve from 0.01481\n",
      "\n",
      "Epoch 00566: loss improved from 0.01481 to 0.01481, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00567: loss improved from 0.01481 to 0.01480, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00568: loss improved from 0.01480 to 0.01479, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00569: loss improved from 0.01479 to 0.01478, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00570: loss improved from 0.01478 to 0.01478, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00571: loss improved from 0.01478 to 0.01478, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00572: loss did not improve from 0.01478\n",
      "\n",
      "Epoch 00573: loss did not improve from 0.01478\n",
      "\n",
      "Epoch 00574: loss did not improve from 0.01478\n",
      "\n",
      "Epoch 00575: loss did not improve from 0.01478\n",
      "\n",
      "Epoch 00576: loss did not improve from 0.01478\n",
      "\n",
      "Epoch 00577: loss did not improve from 0.01478\n",
      "\n",
      "Epoch 00578: loss improved from 0.01478 to 0.01477, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00579: loss improved from 0.01477 to 0.01476, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00580: loss improved from 0.01476 to 0.01476, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00581: loss improved from 0.01476 to 0.01476, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00582: loss improved from 0.01476 to 0.01476, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00583: loss improved from 0.01476 to 0.01476, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00584: loss improved from 0.01476 to 0.01476, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00585: loss improved from 0.01476 to 0.01475, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00586: loss improved from 0.01475 to 0.01475, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00587: loss improved from 0.01475 to 0.01474, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00588: loss improved from 0.01474 to 0.01474, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00589: loss improved from 0.01474 to 0.01474, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00590: loss did not improve from 0.01474\n",
      "\n",
      "Epoch 00591: loss improved from 0.01474 to 0.01474, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00592: loss improved from 0.01474 to 0.01474, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00593: loss improved from 0.01474 to 0.01473, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00594: loss improved from 0.01473 to 0.01473, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00595: loss improved from 0.01473 to 0.01473, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00596: loss improved from 0.01473 to 0.01472, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00597: loss improved from 0.01472 to 0.01472, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00598: loss improved from 0.01472 to 0.01471, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00599: loss improved from 0.01471 to 0.01471, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00600: loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00601: loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00602: loss did not improve from 0.01471\n",
      "\n",
      "Epoch 00603: loss improved from 0.01471 to 0.01471, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00604: loss improved from 0.01471 to 0.01471, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00605: loss improved from 0.01471 to 0.01470, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00606: loss improved from 0.01470 to 0.01469, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00607: loss improved from 0.01469 to 0.01469, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00608: loss improved from 0.01469 to 0.01469, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00609: loss improved from 0.01469 to 0.01469, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00610: loss did not improve from 0.01469\n",
      "\n",
      "Epoch 00611: loss improved from 0.01469 to 0.01469, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00612: loss improved from 0.01469 to 0.01468, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00613: loss improved from 0.01468 to 0.01468, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00614: loss improved from 0.01468 to 0.01467, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00615: loss improved from 0.01467 to 0.01467, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00616: loss improved from 0.01467 to 0.01467, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00617: loss improved from 0.01467 to 0.01467, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00618: loss did not improve from 0.01467\n",
      "\n",
      "Epoch 00619: loss improved from 0.01467 to 0.01467, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00620: loss improved from 0.01467 to 0.01467, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00621: loss improved from 0.01467 to 0.01466, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00622: loss improved from 0.01466 to 0.01466, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00623: loss improved from 0.01466 to 0.01465, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00624: loss improved from 0.01465 to 0.01465, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00625: loss improved from 0.01465 to 0.01465, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00626: loss improved from 0.01465 to 0.01464, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00627: loss did not improve from 0.01464\n",
      "\n",
      "Epoch 00628: loss did not improve from 0.01464\n",
      "\n",
      "Epoch 00629: loss improved from 0.01464 to 0.01464, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00630: loss improved from 0.01464 to 0.01463, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00631: loss improved from 0.01463 to 0.01463, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00632: loss improved from 0.01463 to 0.01462, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00633: loss improved from 0.01462 to 0.01462, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00634: loss did not improve from 0.01462\n",
      "\n",
      "Epoch 00635: loss did not improve from 0.01462\n",
      "\n",
      "Epoch 00636: loss did not improve from 0.01462\n",
      "\n",
      "Epoch 00637: loss did not improve from 0.01462\n",
      "\n",
      "Epoch 00638: loss improved from 0.01462 to 0.01462, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00639: loss improved from 0.01462 to 0.01461, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00640: loss improved from 0.01461 to 0.01461, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00641: loss improved from 0.01461 to 0.01460, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00642: loss improved from 0.01460 to 0.01460, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00643: loss improved from 0.01460 to 0.01460, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00644: loss did not improve from 0.01460\n",
      "\n",
      "Epoch 00645: loss improved from 0.01460 to 0.01459, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00646: loss improved from 0.01459 to 0.01459, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00647: loss improved from 0.01459 to 0.01458, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00648: loss improved from 0.01458 to 0.01458, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00649: loss improved from 0.01458 to 0.01458, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00650: loss improved from 0.01458 to 0.01458, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00651: loss improved from 0.01458 to 0.01458, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00652: loss improved from 0.01458 to 0.01457, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00653: loss improved from 0.01457 to 0.01457, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00654: loss improved from 0.01457 to 0.01457, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00655: loss improved from 0.01457 to 0.01456, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00656: loss improved from 0.01456 to 0.01456, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00657: loss improved from 0.01456 to 0.01455, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00658: loss improved from 0.01455 to 0.01455, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00659: loss improved from 0.01455 to 0.01454, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00660: loss improved from 0.01454 to 0.01454, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00661: loss improved from 0.01454 to 0.01454, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00662: loss improved from 0.01454 to 0.01453, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00663: loss did not improve from 0.01453\n",
      "\n",
      "Epoch 00664: loss improved from 0.01453 to 0.01453, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00665: loss improved from 0.01453 to 0.01453, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00666: loss improved from 0.01453 to 0.01452, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00667: loss did not improve from 0.01452\n",
      "\n",
      "Epoch 00668: loss improved from 0.01452 to 0.01451, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00669: loss improved from 0.01451 to 0.01450, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00670: loss improved from 0.01450 to 0.01450, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00671: loss improved from 0.01450 to 0.01449, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00672: loss did not improve from 0.01449\n",
      "\n",
      "Epoch 00673: loss did not improve from 0.01449\n",
      "\n",
      "Epoch 00674: loss did not improve from 0.01449\n",
      "\n",
      "Epoch 00675: loss did not improve from 0.01449\n",
      "\n",
      "Epoch 00676: loss did not improve from 0.01449\n",
      "\n",
      "Epoch 00677: loss improved from 0.01449 to 0.01449, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00678: loss improved from 0.01449 to 0.01448, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00679: loss improved from 0.01448 to 0.01447, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00680: loss improved from 0.01447 to 0.01447, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00681: loss improved from 0.01447 to 0.01446, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00682: loss did not improve from 0.01446\n",
      "\n",
      "Epoch 00683: loss did not improve from 0.01446\n",
      "\n",
      "Epoch 00684: loss did not improve from 0.01446\n",
      "\n",
      "Epoch 00685: loss did not improve from 0.01446\n",
      "\n",
      "Epoch 00686: loss improved from 0.01446 to 0.01446, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00687: loss improved from 0.01446 to 0.01446, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00688: loss improved from 0.01446 to 0.01445, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00689: loss improved from 0.01445 to 0.01445, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00690: loss improved from 0.01445 to 0.01445, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00691: loss improved from 0.01445 to 0.01444, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00692: loss improved from 0.01444 to 0.01444, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00693: loss improved from 0.01444 to 0.01444, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00694: loss improved from 0.01444 to 0.01443, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00695: loss improved from 0.01443 to 0.01443, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00696: loss improved from 0.01443 to 0.01443, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00697: loss improved from 0.01443 to 0.01442, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00698: loss improved from 0.01442 to 0.01442, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00699: loss improved from 0.01442 to 0.01442, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00700: loss improved from 0.01442 to 0.01441, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00701: loss improved from 0.01441 to 0.01441, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00702: loss improved from 0.01441 to 0.01441, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00703: loss improved from 0.01441 to 0.01440, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00704: loss improved from 0.01440 to 0.01440, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00705: loss improved from 0.01440 to 0.01440, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00706: loss improved from 0.01440 to 0.01439, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00707: loss improved from 0.01439 to 0.01439, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00708: loss improved from 0.01439 to 0.01439, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00709: loss improved from 0.01439 to 0.01439, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00710: loss improved from 0.01439 to 0.01438, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00711: loss improved from 0.01438 to 0.01438, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00712: loss improved from 0.01438 to 0.01437, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00713: loss improved from 0.01437 to 0.01437, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00714: loss improved from 0.01437 to 0.01436, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00715: loss improved from 0.01436 to 0.01436, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00716: loss did not improve from 0.01436\n",
      "\n",
      "Epoch 00717: loss did not improve from 0.01436\n",
      "\n",
      "Epoch 00718: loss improved from 0.01436 to 0.01436, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00719: loss improved from 0.01436 to 0.01435, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00720: loss improved from 0.01435 to 0.01435, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00721: loss improved from 0.01435 to 0.01434, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00722: loss improved from 0.01434 to 0.01434, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00723: loss improved from 0.01434 to 0.01433, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00724: loss improved from 0.01433 to 0.01433, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00725: loss improved from 0.01433 to 0.01433, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00726: loss improved from 0.01433 to 0.01433, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00727: loss improved from 0.01433 to 0.01432, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00728: loss improved from 0.01432 to 0.01432, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00729: loss improved from 0.01432 to 0.01432, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00730: loss improved from 0.01432 to 0.01431, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00731: loss improved from 0.01431 to 0.01431, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00732: loss improved from 0.01431 to 0.01431, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00733: loss improved from 0.01431 to 0.01430, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00734: loss improved from 0.01430 to 0.01430, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00735: loss improved from 0.01430 to 0.01429, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00736: loss improved from 0.01429 to 0.01429, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00737: loss improved from 0.01429 to 0.01428, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00738: loss improved from 0.01428 to 0.01428, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00739: loss improved from 0.01428 to 0.01428, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00740: loss improved from 0.01428 to 0.01427, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00741: loss improved from 0.01427 to 0.01427, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00742: loss improved from 0.01427 to 0.01427, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00743: loss improved from 0.01427 to 0.01426, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00744: loss improved from 0.01426 to 0.01426, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00745: loss improved from 0.01426 to 0.01426, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00746: loss improved from 0.01426 to 0.01425, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00747: loss improved from 0.01425 to 0.01425, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00748: loss improved from 0.01425 to 0.01425, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00749: loss improved from 0.01425 to 0.01424, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00750: loss improved from 0.01424 to 0.01424, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00751: loss improved from 0.01424 to 0.01423, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00752: loss did not improve from 0.01423\n",
      "\n",
      "Epoch 00753: loss improved from 0.01423 to 0.01423, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00754: loss improved from 0.01423 to 0.01422, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00755: loss improved from 0.01422 to 0.01422, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00756: loss improved from 0.01422 to 0.01422, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00757: loss improved from 0.01422 to 0.01421, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00758: loss improved from 0.01421 to 0.01421, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00759: loss improved from 0.01421 to 0.01421, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00760: loss improved from 0.01421 to 0.01420, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00761: loss improved from 0.01420 to 0.01420, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00762: loss improved from 0.01420 to 0.01420, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00763: loss improved from 0.01420 to 0.01419, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00764: loss improved from 0.01419 to 0.01419, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00765: loss improved from 0.01419 to 0.01419, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00766: loss improved from 0.01419 to 0.01418, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00767: loss improved from 0.01418 to 0.01417, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00768: loss improved from 0.01417 to 0.01417, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00769: loss improved from 0.01417 to 0.01417, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00770: loss did not improve from 0.01417\n",
      "\n",
      "Epoch 00771: loss did not improve from 0.01417\n",
      "\n",
      "Epoch 00772: loss did not improve from 0.01417\n",
      "\n",
      "Epoch 00773: loss improved from 0.01417 to 0.01417, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00774: loss improved from 0.01417 to 0.01416, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00775: loss improved from 0.01416 to 0.01415, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00776: loss improved from 0.01415 to 0.01415, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00777: loss improved from 0.01415 to 0.01414, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00778: loss improved from 0.01414 to 0.01414, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00779: loss improved from 0.01414 to 0.01413, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00780: loss improved from 0.01413 to 0.01413, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00781: loss improved from 0.01413 to 0.01413, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00782: loss improved from 0.01413 to 0.01413, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00783: loss improved from 0.01413 to 0.01412, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00784: loss improved from 0.01412 to 0.01412, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00785: loss improved from 0.01412 to 0.01411, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00786: loss did not improve from 0.01411\n",
      "\n",
      "Epoch 00787: loss improved from 0.01411 to 0.01411, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00788: loss did not improve from 0.01411\n",
      "\n",
      "Epoch 00789: loss improved from 0.01411 to 0.01411, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00790: loss improved from 0.01411 to 0.01410, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00791: loss improved from 0.01410 to 0.01410, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00792: loss improved from 0.01410 to 0.01409, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00793: loss improved from 0.01409 to 0.01409, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00794: loss improved from 0.01409 to 0.01408, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00795: loss improved from 0.01408 to 0.01408, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00796: loss improved from 0.01408 to 0.01408, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00797: loss improved from 0.01408 to 0.01408, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00798: loss improved from 0.01408 to 0.01407, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00799: loss improved from 0.01407 to 0.01407, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00800: loss improved from 0.01407 to 0.01407, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00801: loss improved from 0.01407 to 0.01406, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00802: loss improved from 0.01406 to 0.01406, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00803: loss improved from 0.01406 to 0.01406, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00804: loss did not improve from 0.01406\n",
      "\n",
      "Epoch 00805: loss improved from 0.01406 to 0.01405, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00806: loss did not improve from 0.01405\n",
      "\n",
      "Epoch 00807: loss improved from 0.01405 to 0.01405, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00808: loss improved from 0.01405 to 0.01404, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00809: loss improved from 0.01404 to 0.01404, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00810: loss improved from 0.01404 to 0.01404, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00811: loss improved from 0.01404 to 0.01403, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00812: loss improved from 0.01403 to 0.01403, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00813: loss improved from 0.01403 to 0.01403, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00814: loss did not improve from 0.01403\n",
      "\n",
      "Epoch 00815: loss improved from 0.01403 to 0.01402, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00816: loss improved from 0.01402 to 0.01402, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00817: loss improved from 0.01402 to 0.01402, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00818: loss improved from 0.01402 to 0.01401, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00819: loss improved from 0.01401 to 0.01401, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00820: loss improved from 0.01401 to 0.01400, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00821: loss improved from 0.01400 to 0.01400, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00822: loss improved from 0.01400 to 0.01400, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00823: loss improved from 0.01400 to 0.01399, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00824: loss improved from 0.01399 to 0.01399, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00825: loss improved from 0.01399 to 0.01399, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00826: loss improved from 0.01399 to 0.01399, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00827: loss improved from 0.01399 to 0.01398, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00828: loss improved from 0.01398 to 0.01398, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00829: loss improved from 0.01398 to 0.01397, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00830: loss improved from 0.01397 to 0.01397, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00831: loss improved from 0.01397 to 0.01397, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00832: loss improved from 0.01397 to 0.01397, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00833: loss improved from 0.01397 to 0.01396, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00834: loss improved from 0.01396 to 0.01396, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00835: loss improved from 0.01396 to 0.01395, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00836: loss improved from 0.01395 to 0.01395, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00837: loss improved from 0.01395 to 0.01395, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00838: loss improved from 0.01395 to 0.01394, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00839: loss improved from 0.01394 to 0.01394, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00840: loss improved from 0.01394 to 0.01394, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00841: loss improved from 0.01394 to 0.01393, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00842: loss improved from 0.01393 to 0.01393, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00843: loss improved from 0.01393 to 0.01392, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00844: loss improved from 0.01392 to 0.01392, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00845: loss improved from 0.01392 to 0.01392, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00846: loss improved from 0.01392 to 0.01392, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00847: loss improved from 0.01392 to 0.01391, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00848: loss improved from 0.01391 to 0.01391, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00849: loss improved from 0.01391 to 0.01391, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00850: loss improved from 0.01391 to 0.01390, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00851: loss improved from 0.01390 to 0.01389, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00852: loss improved from 0.01389 to 0.01389, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00853: loss improved from 0.01389 to 0.01388, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00854: loss improved from 0.01388 to 0.01388, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00855: loss improved from 0.01388 to 0.01388, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00856: loss improved from 0.01388 to 0.01388, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00857: loss improved from 0.01388 to 0.01388, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00858: loss improved from 0.01388 to 0.01387, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00859: loss improved from 0.01387 to 0.01387, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00860: loss improved from 0.01387 to 0.01387, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00861: loss improved from 0.01387 to 0.01386, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00862: loss improved from 0.01386 to 0.01386, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00863: loss improved from 0.01386 to 0.01386, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00864: loss improved from 0.01386 to 0.01385, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00865: loss improved from 0.01385 to 0.01385, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00866: loss improved from 0.01385 to 0.01384, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00867: loss improved from 0.01384 to 0.01384, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00868: loss improved from 0.01384 to 0.01384, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00869: loss improved from 0.01384 to 0.01383, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00870: loss improved from 0.01383 to 0.01383, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00871: loss improved from 0.01383 to 0.01383, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00872: loss improved from 0.01383 to 0.01383, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00873: loss improved from 0.01383 to 0.01382, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00874: loss improved from 0.01382 to 0.01382, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00875: loss improved from 0.01382 to 0.01381, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00876: loss improved from 0.01381 to 0.01381, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00877: loss improved from 0.01381 to 0.01381, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00878: loss improved from 0.01381 to 0.01381, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00879: loss improved from 0.01381 to 0.01380, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00880: loss improved from 0.01380 to 0.01380, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00881: loss improved from 0.01380 to 0.01380, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00882: loss improved from 0.01380 to 0.01379, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00883: loss improved from 0.01379 to 0.01379, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00884: loss improved from 0.01379 to 0.01379, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00885: loss improved from 0.01379 to 0.01379, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00886: loss improved from 0.01379 to 0.01378, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00887: loss improved from 0.01378 to 0.01378, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00888: loss improved from 0.01378 to 0.01378, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00889: loss improved from 0.01378 to 0.01377, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00890: loss improved from 0.01377 to 0.01377, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00891: loss improved from 0.01377 to 0.01377, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00892: loss improved from 0.01377 to 0.01376, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00893: loss improved from 0.01376 to 0.01376, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00894: loss improved from 0.01376 to 0.01376, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00895: loss improved from 0.01376 to 0.01376, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00896: loss improved from 0.01376 to 0.01375, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00897: loss improved from 0.01375 to 0.01375, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00898: loss improved from 0.01375 to 0.01375, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00899: loss improved from 0.01375 to 0.01374, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00900: loss improved from 0.01374 to 0.01374, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00901: loss improved from 0.01374 to 0.01374, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00902: loss improved from 0.01374 to 0.01374, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00903: loss improved from 0.01374 to 0.01373, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00904: loss improved from 0.01373 to 0.01373, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00905: loss improved from 0.01373 to 0.01373, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00906: loss improved from 0.01373 to 0.01373, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00907: loss improved from 0.01373 to 0.01373, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00908: loss improved from 0.01373 to 0.01372, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00909: loss improved from 0.01372 to 0.01372, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00910: loss improved from 0.01372 to 0.01372, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00911: loss improved from 0.01372 to 0.01371, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00912: loss improved from 0.01371 to 0.01371, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00913: loss improved from 0.01371 to 0.01371, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00914: loss improved from 0.01371 to 0.01371, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00915: loss improved from 0.01371 to 0.01371, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00916: loss improved from 0.01371 to 0.01370, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00917: loss improved from 0.01370 to 0.01370, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00918: loss improved from 0.01370 to 0.01370, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00919: loss improved from 0.01370 to 0.01370, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00920: loss improved from 0.01370 to 0.01369, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00921: loss improved from 0.01369 to 0.01369, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00922: loss improved from 0.01369 to 0.01369, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00923: loss improved from 0.01369 to 0.01369, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00924: loss improved from 0.01369 to 0.01368, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00925: loss improved from 0.01368 to 0.01368, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00926: loss improved from 0.01368 to 0.01368, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00927: loss improved from 0.01368 to 0.01368, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00928: loss improved from 0.01368 to 0.01367, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00929: loss improved from 0.01367 to 0.01367, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00930: loss improved from 0.01367 to 0.01367, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00931: loss improved from 0.01367 to 0.01367, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00932: loss improved from 0.01367 to 0.01367, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00933: loss improved from 0.01367 to 0.01366, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00934: loss improved from 0.01366 to 0.01366, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00935: loss improved from 0.01366 to 0.01366, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00936: loss improved from 0.01366 to 0.01366, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00937: loss improved from 0.01366 to 0.01365, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00938: loss improved from 0.01365 to 0.01365, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00939: loss improved from 0.01365 to 0.01365, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00940: loss improved from 0.01365 to 0.01365, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00941: loss improved from 0.01365 to 0.01365, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00942: loss improved from 0.01365 to 0.01364, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00943: loss improved from 0.01364 to 0.01364, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00944: loss improved from 0.01364 to 0.01364, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00945: loss improved from 0.01364 to 0.01364, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00946: loss improved from 0.01364 to 0.01363, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00947: loss improved from 0.01363 to 0.01363, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00948: loss improved from 0.01363 to 0.01363, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00949: loss improved from 0.01363 to 0.01363, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00950: loss improved from 0.01363 to 0.01363, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00951: loss improved from 0.01363 to 0.01362, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00952: loss improved from 0.01362 to 0.01362, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00953: loss improved from 0.01362 to 0.01362, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00954: loss improved from 0.01362 to 0.01362, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00955: loss improved from 0.01362 to 0.01361, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00956: loss improved from 0.01361 to 0.01361, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00957: loss improved from 0.01361 to 0.01361, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00958: loss improved from 0.01361 to 0.01361, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00959: loss improved from 0.01361 to 0.01360, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00960: loss improved from 0.01360 to 0.01360, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00961: loss improved from 0.01360 to 0.01360, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00962: loss improved from 0.01360 to 0.01360, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00963: loss improved from 0.01360 to 0.01360, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00964: loss improved from 0.01360 to 0.01359, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00965: loss improved from 0.01359 to 0.01359, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00966: loss improved from 0.01359 to 0.01359, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00967: loss improved from 0.01359 to 0.01359, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00968: loss improved from 0.01359 to 0.01359, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00969: loss improved from 0.01359 to 0.01358, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00970: loss improved from 0.01358 to 0.01358, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00971: loss improved from 0.01358 to 0.01358, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00972: loss improved from 0.01358 to 0.01358, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00973: loss improved from 0.01358 to 0.01357, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00974: loss improved from 0.01357 to 0.01357, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00975: loss improved from 0.01357 to 0.01357, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00976: loss improved from 0.01357 to 0.01357, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00977: loss improved from 0.01357 to 0.01357, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00978: loss did not improve from 0.01357\n",
      "\n",
      "Epoch 00979: loss improved from 0.01357 to 0.01356, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00980: loss improved from 0.01356 to 0.01356, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00981: loss improved from 0.01356 to 0.01356, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00982: loss improved from 0.01356 to 0.01356, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00983: loss improved from 0.01356 to 0.01355, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00984: loss improved from 0.01355 to 0.01355, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00985: loss improved from 0.01355 to 0.01355, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00986: loss improved from 0.01355 to 0.01355, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00987: loss improved from 0.01355 to 0.01355, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00988: loss improved from 0.01355 to 0.01355, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00989: loss improved from 0.01355 to 0.01354, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00990: loss did not improve from 0.01354\n",
      "\n",
      "Epoch 00991: loss improved from 0.01354 to 0.01354, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00992: loss improved from 0.01354 to 0.01354, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00993: loss improved from 0.01354 to 0.01353, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00994: loss improved from 0.01353 to 0.01353, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00995: loss improved from 0.01353 to 0.01353, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00996: loss improved from 0.01353 to 0.01353, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00997: loss improved from 0.01353 to 0.01353, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00998: loss improved from 0.01353 to 0.01353, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 00999: loss improved from 0.01353 to 0.01352, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n",
      "\n",
      "Epoch 01000: loss improved from 0.01352 to 0.01352, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part1_lstmm_weights-best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163bd7c0a58>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run your model!\n",
    "checkpointfile_rnn1 = 'rnn_part1_lstmm_weights-best.hdf5'\n",
    "callbacks_rnn = Part1_RNN().getcallbacks(checkpointstr=checkpointfile_rnn1,\n",
    "                                         eval_cb=None)\n",
    "\n",
    "# model.load_weights('checkpoints/rnn_part1_lstmm_weights-best.hdf5')\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1000, \n",
    "          batch_size=88, verbose=0, \n",
    "          callbacks=callbacks_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5  Checking model performance\n",
    "\n",
    "With your model fit we can now make predictions on both our training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:14.890726Z",
     "start_time": "2018-06-08T15:43:13.868281Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate predictions for training\n",
    "train_predict = model.predict(X_train, batch_size=88)\n",
    "\n",
    "test_model = build_part1_RNN(windowsize=7)\n",
    "test_model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
    "test_model.load_weights('checkpoints/rnn_part1_lstmm_weights-best.hdf5')\n",
    "\n",
    "test_predict = test_model.predict(X_test, batch_size=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we compute training and testing errors using our trained model - you should be able to achieve at least\n",
    "\n",
    "*training_error* < 0.02\n",
    "\n",
    "and \n",
    "\n",
    "*testing_error* < 0.02\n",
    "\n",
    "with your fully trained model.  \n",
    "\n",
    "If either or both of your accuracies are larger than 0.02 re-train your model - increasing the number of epochs you take (a maximum of around 1,000 should do the job) and/or adjusting your batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:16.570125Z",
     "start_time": "2018-06-08T15:43:16.085862Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error = [0.01352131087332964, 0.01352131087332964]\n",
      "testing error = [0.020838681608438492, 0.020838681608438492]\n"
     ]
    }
   ],
   "source": [
    "# print out training and testing errors\n",
    "training_error = model.evaluate(X_train, y_train, batch_size=88, verbose=0)\n",
    "print('training error = ' + str(training_error))\n",
    "\n",
    "testing_error = test_model.evaluate(X_test, y_test, batch_size=43, verbose=0)\n",
    "print('testing error = ' + str(testing_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activating the next cell plots the original data, as well as both predictions on the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:18.024027Z",
     "start_time": "2018-06-08T15:43:17.762297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Plot everything - the original series as well as predictions on training and testing sets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot original series\n",
    "plt.plot(dataset,color = 'k')\n",
    "\n",
    "# plot training set prediction\n",
    "split_pt = train_test_split + window_size \n",
    "plt.plot(np.arange(window_size,split_pt,1),train_predict,color = 'b')\n",
    "\n",
    "# plot testing set prediction\n",
    "plt.plot(np.arange(split_pt,split_pt + len(test_predict),1),test_predict,color = 'r')\n",
    "\n",
    "# pretty up graph\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('(normalized) price of Apple stock')\n",
    "plt.legend(['original series','training fit','testing fit'],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** you can try out any time series for this exercise!  If you would like to try another see e.g., [this site containing thousands of time series](https://datamarket.com/data/list/?q=provider%3Atsdl) and pick another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Create a sequence generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1  Getting started\n",
    "\n",
    "In this project you will implement a popular Recurrent Neural Network (RNN) architecture to create an English language sequence generator capable of building semi-coherent English sentences from scratch by building them up character-by-character.  This will require a substantial amount amount of parameter tuning on a large training corpus (at least 100,000 characters long).  In particular for this project we will be using a complete version of Sir Arthur Conan Doyle's classic book The Adventures of Sherlock Holmes.\n",
    "\n",
    "How can we train a machine learning model to generate text automatically, character-by-character?  *By showing the model many training examples so it can learn a pattern between input and output.*  With this type of text generation each input is a string of valid characters like this one\n",
    "\n",
    "*dogs are grea*\n",
    "\n",
    "while the corresponding output is the next character in the sentence - which here is 't' (since the complete sentence is 'dogs are great').  We need to show a model many such examples in order for it to make reasonable predictions.\n",
    "\n",
    "**Fun note:** For those interested in how text generation is being used check out some of the following fun resources:\n",
    "\n",
    "- [Generate wacky sentences](http://www.cs.toronto.edu/~ilya/rnn.html) with this academic RNN text generator\n",
    "\n",
    "- Various twitter bots that tweet automatically generated text like[this one](http://tweet-generator-alex.herokuapp.com/).\n",
    "\n",
    "- the [NanoGenMo](https://github.com/NaNoGenMo/2016) annual contest to automatically produce a 50,000+ novel automatically\n",
    "\n",
    "- [Robot Shakespeare](https://github.com/genekogan/RobotShakespeare) a text generator that automatically produces Shakespear-esk sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  Preprocessing a text dataset\n",
    "\n",
    "Our first task is to get a large text corpus for use in training, and on it we perform a several light pre-processing tasks.  The default corpus we will use is the classic book Sherlock Holmes, but you can use a variety of others as well - so long as they are fairly large (around 100,000 characters or more).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:34.902971Z",
     "start_time": "2018-06-08T15:43:34.732524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our original text has 581881 characters\n"
     ]
    }
   ],
   "source": [
    "# read in the text, transforming everything to lower case\n",
    "text = open('datasets/holmes.txt').read().lower()\n",
    "print('our original text has ' + str(len(text)) + ' characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets examine a bit of the raw text.  Because we are interested in creating sentences of English words automatically by building up each word character-by-character, we only want to train on valid English words.  In other words - we need to remove all of the other characters that are not part of English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:36.269580Z",
     "start_time": "2018-06-08T15:43:36.110154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"in his eyes she eclipses\\nand predominates the whole of her sex. it was not that he felt\\nany emotion akin to love for irene adler. all emotions, and that\\none particularly, were abhorrent to his cold, precise but\\nadmirably balanced mind. he was, i take it, the most perfect\\nreasoning and observing machine that the world has seen, but as a\\nlover he would have placed himself in a false position. he never\\nspoke of the softer passions, save with a gibe and a sneer. they\\nwere admirable things for the observer--excellent for drawing the\\nveil from men's motives and actions. but for the trained reasoner\\nto admit such intrusions into his own delicate and finely\\nadjusted temperament was to introduce a di\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### print out the first 1000 characters of the raw text to get a sense of what we need to throw out\n",
    "text[1300:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - there's a lot of junk here (i.e., weird uncommon character combinations - as this first character chunk contains the title and author page, as well as table of contents)!  To keep things simple, we want to train our RNN on a large chunk of more typical English sentences - we don't want it to start thinking non-english words or strange characters are valid! - so lets clean up the data a bit.\n",
    "\n",
    "First, since the dataset is so large and the first few hundred characters contain a lot of junk, lets cut it out.  Lets also find-and-replace those newline tags with empty spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how the first 1000 characters of our text looks now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:37.675317Z",
     "start_time": "2018-06-08T15:43:37.504888Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaned_text_nb(text):\n",
    "    from string import ascii_lowercase, whitespace, digits\n",
    "    punctuation = ['!', ',', '.', ':', ';', '?']\n",
    "    filter_ = ascii_lowercase+''.join(punctuation)+\"`' \"+'\"'\n",
    "    text = [' ' if(x in whitespace) else x for x in text]\n",
    "    text = [' ' if(x not in filter_) else x for x in text]\n",
    "    text  = ''.join(text).split()\n",
    "    return ' '.join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:39.089691Z",
     "start_time": "2018-06-08T15:43:38.913220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"in his eyes she eclipses and predominates the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. he was, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. he never spoke of the softer passions, save with a gibe and a sneer. they were admirable things for the observer excellent for drawing the veil from men's motives and actions. but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. grit in a sensitive instrument, or a crack in one of his own high power lenses, would not be more disturbing than a strong emotion in a nature such as his. and yet there was but one woman to him, and that woman was the late iren\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### find and replace '\\n' and '\\r' symbols - replacing them \n",
    "# replacing '\\n' with '' simply removes the sequence\n",
    "### print out the first 1000 characters of the raw text to get a sense of what we need to throw out\n",
    "\n",
    "text = text[1300:]\n",
    "cleaned_text_nb(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_3'></a>\n",
    "\n",
    "#### TODO: finish cleaning the text\n",
    "\n",
    "Lets make sure we haven't left any other atypical characters (commas, periods, etc., are ok) lurking around in the depths of the text.  You can do this by enumerating all the text's unique characters, examining them, and then replacing any unwanted characters with empty spaces!  Once we find all of the text's unique characters, we can remove all of the atypical ones in the next cell.  Note: don't remove the punctuation marks given in my_answers.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:40.573409Z",
     "start_time": "2018-06-08T15:43:40.318708Z"
    }
   },
   "outputs": [],
   "source": [
    "### TODO: implement cleaned_text in my_answers.py\n",
    "from my_answers import cleaned_text\n",
    "\n",
    "text = cleaned_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your chosen characters removed print out the first few hundred lines again just to double check that everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:41.918962Z",
     "start_time": "2018-06-08T15:43:41.752519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in his eyes she eclipses and predominates the whole of her sex. it was not that he felt any emotion akin to love for irene adler. all emotions, and that one particularly, were abhorrent to his cold, precise but admirably balanced mind. he was, i take it, the most perfect reasoning and observing machine that the world has seen, but as a lover he would have placed himself in a false position. he never spoke of the softer passions, save with a gibe and a sneer. they were admirable things for the observer excellent for drawing the veil from men s motives and actions. but for the trained reasoner to admit such intrusions into his own delicate and finely adjusted temperament was to introduce a distracting factor which might throw a doubt upon all his mental results. grit in a sensitive instrument, or a crack in one of his own high power lenses, would not be more disturbing than a strong emotion in a nature such as his. and yet there was but one woman to him, and that woman was the late irene adler, of dubious and questionable memory. i had seen little of holmes lately. my marriage had drifted us away from each other. my own complete happiness, and the home centred interests which rise up around the man who first finds himself master of his own establishment, were sufficient to absorb all my attention, while holmes, who loathed every form of society with his whole bohemian soul, remained in our lodgings in baker street, buried among his old books, and alternating from week to week between cocaine and ambition, the drowsiness of the drug, and the fierce energy of his own keen nature. he was still, as ever, deeply attracted by the study of crime, and occupied his immense faculties and extraordinary powers of observation in following out those clues, and clearing up those mysteries which had been abandoned as hopeless by the official police. from time to time i heard some vague account of his doings: of his summons to odessa in the case of the trepoff murder, of his clearing '"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### print out the first 2000 characters of the raw text to get a sense of what we need to throw out\n",
    "text[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have thrown out a good number of non-English characters/character sequences lets print out some statistics about the dataset - including number of total characters and number of unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:43.268553Z",
     "start_time": "2018-06-08T15:43:43.094087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has 570574 total number of characters\n",
      "this corpus has 33 unique characters\n"
     ]
    }
   ],
   "source": [
    "# count the number of unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# print some of the text, as well as statistics\n",
    "print (\"this corpus has \" +  str(len(text)) + \" total number of characters\")\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  Cutting data into input/output pairs\n",
    "\n",
    "Now that we have our text all cleaned up, how can we use it to train a model to generate sentences automatically?  First we need to train a machine learning model - and in order to do that we need a set of input/output pairs for a model to train on.  How can we create a set of input/output pairs from our text to train on?\n",
    "\n",
    "Remember in part 1 of this notebook how we used a sliding window to extract input/output pairs from a time series?  We do the same thing here!  We slide a window of length $T$ along our giant text corpus - everything in the window becomes one input while the character following becomes its corresponding output.  This process of extracting input/output pairs is illustrated in the gif below on a small example text using a window size of T = 5.\n",
    "\n",
    "<img src=\"images/text_windowing_training.gif\" width=400 height=400/>\n",
    "\n",
    "Notice one aspect of the sliding window in this gif that does not mirror the analogous gif for time series shown in part 1 of the notebook - we do not need to slide the window along one character at a time but can move by a fixed step size $M$ greater than 1 (in the gif indeed $M = 1$).  This is done with large input texts (like ours which has over 500,000 characters!) when sliding the window along one character at a time we would create far too many input/output pairs to be able to reasonably compute with.\n",
    "\n",
    "More formally lets denote our text corpus - which is one long string of characters - as follows\n",
    "\n",
    "$$s_{0},s_{1},s_{2},...,s_{P}$$\n",
    "\n",
    "where $P$ is the length of the text (again for our text $P \\approx 500,000!$).  Sliding a window of size T = 5 with a step length of M = 1 (these are the parameters shown in the gif above) over this sequence produces the following list of input/output pairs\n",
    "\n",
    "\n",
    "$$\\begin{array}{c|c}\n",
    "\\text{Input} & \\text{Output}\\\\\n",
    "\\hline \\color{CornflowerBlue} {\\langle s_{1},s_{2},s_{3},s_{4},s_{5}\\rangle} & \\color{Goldenrod}{ s_{6}} \\\\\n",
    "\\ \\color{CornflowerBlue} {\\langle s_{2},s_{3},s_{4},s_{5},s_{6} \\rangle } & \\color{Goldenrod} {s_{7} } \\\\\n",
    "\\color{CornflowerBlue}  {\\vdots} & \\color{Goldenrod} {\\vdots}\\\\\n",
    "\\color{CornflowerBlue} { \\langle s_{P-5},s_{P-4},s_{P-3},s_{P-2},s_{P-1} \\rangle } & \\color{Goldenrod} {s_{P}}\n",
    "\\end{array}$$\n",
    "\n",
    "Notice here that each input is a sequence (or vector) of 5 characters (and in general has length equal to the window size T) while each corresponding output is a single character.  We created around P total number of input/output pairs  (for general step size M we create around ceil(P/M) pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:44.730232Z",
     "start_time": "2018-06-08T15:43:44.558806Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def window_transform_text_nb(text, wSize=100, stride=5):\n",
    "    inputs, outputs  = [],[]\n",
    "    for _ in range(0, len(text) - wSize, stride):\n",
    "        inputs.append(text[_:_+wSize])\n",
    "        outputs.append(text[_+wSize])\n",
    "        \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_4'></a>\n",
    "\n",
    "Now its time for you to window the input time series as described above! \n",
    "\n",
    "**TODO:** Create a function that runs a sliding window along the input text and creates associated input/output pairs.  A skeleton function has been provided for you.  Note that this function should input a) the text  b) the window size and c) the step size, and return the input/output sequences.  Note: the return items should be *lists* - not numpy arrays.\n",
    "\n",
    "(remember to copy your completed function into the script *my_answers.py* function titled *window_transform_text* before submitting your project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:46.113938Z",
     "start_time": "2018-06-08T15:43:45.958501Z"
    }
   },
   "outputs": [],
   "source": [
    "### TODO: implement window_transform_series in my_answers.py\n",
    "from my_answers import window_transform_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our function complete we can now use it to produce input/output pairs!  We employ the function in the next cell, where the window_size = 50 and step_size = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:47.510627Z",
     "start_time": "2018-06-08T15:43:47.290042Z"
    }
   },
   "outputs": [],
   "source": [
    "# run your text window-ing function \n",
    "window_size = 50\n",
    "step_size = 5\n",
    "inputs, outputs = window_transform_text(text,window_size,step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print out a few input/output pairs to verify that we have made the right sort of stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:48.859226Z",
     "start_time": "2018-06-08T15:43:48.692770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = s she eclipses and predominates the whole of her s\n",
      "output = e\n",
      "--------------\n",
      "input = server excellent for drawing the veil from men s m\n",
      "output = o\n"
     ]
    }
   ],
   "source": [
    "# print out a few of the input/output pairs to verify that we've made the right kind of stuff to learn from\n",
    "print('input = ' + str(inputs[2]))\n",
    "print('output = ' + str(outputs[2]))\n",
    "print('--------------')\n",
    "print('input = ' + str(inputs[100]))\n",
    "print('output = ' + str(outputs[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4  Wait, what kind of problem is text generation again?\n",
    "\n",
    "In part 1 of this notebook we used the same pre-processing technique - the sliding window - to produce a set of training input/output pairs to tackle the problem of time series prediction *by treating the problem as one of regression*.  So what sort of problem do we have here now, with text generation?  Well, the time series prediction was a regression problem because the output (one value of the time series) was a continuous value.  Here - for character-by-character text generation - each output is a *single character*.  This isn't a continuous value - but a distinct class - therefore **character-by-character text generation is a classification problem**.  \n",
    "\n",
    "How many classes are there in the data?  Well, the number of classes is equal to the number of unique characters we have to predict!  How many of those were there in our dataset again?  Lets print out the value again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:50.254280Z",
     "start_time": "2018-06-08T15:43:50.085826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this corpus has 33 unique characters\n",
      "and these characters are \n",
      "[' ', '!', ',', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# print out the number of unique characters in the dataset\n",
    "chars = sorted(list(set(outputs)))\n",
    "print (\"this corpus has \" +  str(len(chars)) + \" unique characters\")\n",
    "print ('and these characters are ')\n",
    "print (chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rockin' - so we have a multiclass classification problem on our hands!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5  One-hot encoding characters\n",
    "\n",
    "The last issue we have to deal with is representing our text data as numerical data so that we can use it as an input to a neural network. One of the conceptually simplest ways of doing this is via a 'one-hot encoding' scheme.  Here's how it works.\n",
    "\n",
    "We transform each character in our inputs/outputs into a vector with length equal to the number of unique characters in our text.  This vector is all zeros except one location where we place a 1 - and this location is unique to each character type.  e.g., we transform 'a', 'b', and 'c' as follows\n",
    "\n",
    "$$a\\longleftarrow\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "0\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right]\\,\\,\\,\\,\\,\\,\\,b\\longleftarrow\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "1\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{array}\\right]\\,\\,\\,\\,\\,c\\longleftarrow\\left[\\begin{array}{c}\n",
    "0\\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "0\\\\\n",
    "0 \n",
    "\\end{array}\\right]\\cdots$$\n",
    "\n",
    "where each vector has 32 entries (or in general: number of entries = number of unique characters in text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first practical step towards doing this one-hot encoding is to form a dictionary mapping each unique character to a unique integer, and one dictionary to do the reverse mapping.  We can then use these dictionaries to quickly make our one-hot encodings, as well as re-translate (from integers to characters) the results of our trained RNN classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:51.634071Z",
     "start_time": "2018-06-08T15:43:51.469506Z"
    }
   },
   "outputs": [],
   "source": [
    "# this dictionary is a function mapping each unique character to a unique integer\n",
    "chars_to_indices = dict((c, i) for i, c in enumerate(chars))  # map each unique character to unique integer\n",
    "\n",
    "# this dictionary is a function mapping each unique integer back to a unique character\n",
    "indices_to_chars = dict((i, c) for i, c in enumerate(chars))  # map each unique integer back to unique character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform our input/output pairs - consisting of characters - to equivalent input/output pairs made up of one-hot encoded vectors.  In the next cell we provide a function for doing just this: it takes in the raw character input/outputs and returns their numerical versions.  In particular the numerical input is given as $\\bf{X}$, and numerical output is given as the $\\bf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:53.017665Z",
     "start_time": "2018-06-08T15:43:52.857223Z"
    }
   },
   "outputs": [],
   "source": [
    "# transform character-based input/output into equivalent numerical versions\n",
    "def encode_io_pairs(text,window_size,step_size):\n",
    "    # number of unique chars\n",
    "    chars = sorted(list(set(text)))\n",
    "    num_chars = len(chars)\n",
    "    \n",
    "    # cut up text into character input/output pairs\n",
    "    inputs, outputs = window_transform_text_nb(text,window_size,step_size)\n",
    "    \n",
    "    # create empty vessels for one-hot encoded input/output\n",
    "    X = np.zeros((len(inputs), window_size, num_chars), dtype=np.bool)\n",
    "    y = np.zeros((len(inputs), num_chars), dtype=np.bool)\n",
    "    \n",
    "    # loop over inputs/outputs and transform and store in X/y\n",
    "    for i, sentence in enumerate(inputs):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, chars_to_indices[char]] = 1\n",
    "        y[i, chars_to_indices[outputs[i]]] = 1\n",
    "        \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the one-hot encoding function by activating the cell below and transform our input/output pairs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:56.519443Z",
     "start_time": "2018-06-08T15:43:54.204780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114095, 100, 33), (114095, 33))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use your function\n",
    "window_size = 100\n",
    "step_size = 5\n",
    "X,y = encode_io_pairs(text,window_size,step_size)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:57.876050Z",
     "start_time": "2018-06-08T15:43:57.713619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362.66196727752686 MiBs.\n"
     ]
    }
   ],
   "source": [
    "print(str((X.size*X.itemsize + y.size*y.itemsize)/(1024**2))+' MiBs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_5'></a>\n",
    "\n",
    "## 2.6 Setting up our RNN\n",
    "\n",
    "With our dataset loaded and the input/output pairs extracted / transformed we can now begin setting up our RNN for training.  Again we will use Keras to quickly build a single hidden layer RNN - where our hidden layer consists of LSTM modules.\n",
    "\n",
    "Time to get to work: build a 3 layer RNN model of the following specification\n",
    "\n",
    "- layer 1 should be an LSTM module with 200 hidden units --> note this should have input_shape = (window_size,len(chars)) where len(chars) = number of unique characters in your cleaned text\n",
    "- layer 2 should be a linear module, fully connected, with len(chars) hidden units --> where len(chars) = number of unique characters in your cleaned text\n",
    "- layer 3 should be a softmax activation ( since we are solving a *multiclass classification*)\n",
    "- Use the **categorical_crossentropy** loss \n",
    "\n",
    "This network can be constructed using just a few lines - as with the RNN network you made in part 1 of this notebook.  See e.g., the [general Keras documentation](https://keras.io/getting-started/sequential-model-guide/) and the [LSTM documentation in particular](https://keras.io/layers/recurrent/) for examples of how to quickly use Keras to build neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:43:59.524434Z",
     "start_time": "2018-06-08T15:43:59.030121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 200)               187200    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 33)                6633      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 33)                0         \n",
      "=================================================================\n",
      "Total params: 193,833\n",
      "Trainable params: 193,833\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from my_answers import Part2_RNN\n",
    "\n",
    "m = build_part2_RNN(windowsize=100, numchars=33)\n",
    "m.summary()\n",
    "del m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7  Training our RNN model for text generation\n",
    "\n",
    "With our RNN setup we can now train it!  Lets begin by trying it out on a small subset of the larger version.  In the next cell we take the first 10,000 input/output pairs from our training database to learn on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:44:02.695178Z",
     "start_time": "2018-06-08T15:44:02.455541Z"
    }
   },
   "outputs": [],
   "source": [
    "# a small subset of our input/output pairs\n",
    "Xsmall = X[1000:11000,:,:].astype(dtype='int')\n",
    "ysmall = y[1000:11000,:].astype(dtype='int')\n",
    "\n",
    "xval = X[11000:12000,:,:].astype(dtype='int')\n",
    "yval = y[11000:12000,:].astype(dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:44:11.895667Z",
     "start_time": "2018-06-08T15:44:03.855263Z"
    }
   },
   "outputs": [],
   "source": [
    "def weightclasses(xseq, yseq):\n",
    "    Sample_tf = OrderedDict({x:1 for x in range(xseq.shape[-1])})\n",
    "    Sample_idf = OrderedDict({x:1 for x in range(xseq.shape[-1])})\n",
    "    Sample_tfidf = OrderedDict({x:1 for x in range(xseq.shape[-1])})\n",
    "    \n",
    "    seqlength = yseq.shape[0]\n",
    "    doclength = xseq[0].shape[0]\n",
    "    dims = (xseq.shape[0],xseq.shape[-1])\n",
    "    yseq  = np.argmax(yseq, axis=-1)\n",
    "    xseq = np.argmax(xseq, axis=-1)\n",
    "\n",
    "    for x_,y_ in zip(xseq,yseq):\n",
    "        Sample_idf[y_] +=1\n",
    "        for keys in x_:\n",
    "                Sample_tf[keys] +=1/doclength\n",
    "    #print('\\n')\n",
    "    for keys in Sample_tf.keys():\n",
    "        Sample_tf[keys] = 1 + np.log10(Sample_tf[keys])\n",
    "        Sample_idf[keys] = np.log10(seqlength/(1 + Sample_idf[keys]))\n",
    "        Sample_tfidf[keys] = Sample_idf[keys] * Sample_tf[keys]\n",
    "        #print(keys, Sample_tf[keys], Sample_idf[keys], Sample_tfidf[keys])\n",
    "    \n",
    "    res = np.array([x/1. for x in Sample_tfidf.values()])\n",
    "    res = res.reshape(dims[1])\n",
    "    return res\n",
    "    \n",
    "\n",
    "Global_weights = weightclasses(X,y)\n",
    "Class_weights = weightclasses(Xsmall,ysmall)\n",
    "Class_weights_val = weightclasses(xval, yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:44:13.327474Z",
     "start_time": "2018-06-08T15:44:13.071796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(sharex=True, sharey=True)\n",
    "ax.plot(Class_weights, color='red' , marker='x', label='Small Set')\n",
    "ax.plot(Class_weights_val, color='blue', marker='*', label='Mini Set')\n",
    "ax.plot(Global_weights, color='cyan', marker='+', label='All Data')\n",
    "plt.xlabel('vocab sequences')\n",
    "plt.ylabel('tfidf scores')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:44:28.164224Z",
     "start_time": "2018-06-08T15:44:27.582650Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from my_answers import build_part2_RNN\n",
    "### necessary functions from the keras library    \n",
    "clear_start(True)\n",
    "\n",
    "\n",
    "# TODO implement build_part2_RNN in my_answers.py\n",
    "checkpointfile_rnn2 = 'rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5'\n",
    "model = build_part2_RNN(windowsize=100, numchars=33)\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = rmsprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# compile model --> make sure initialized optimizer and callbacks - as defined above - are used\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:48:32.337661Z",
     "start_time": "2018-06-08T15:44:31.543040Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 3s 288us/step - loss: 3.0356\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.03560, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 2.8968\n",
      "\n",
      "Epoch 00002: loss improved from 3.03560 to 2.89684, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 2.8710\n",
      "\n",
      "Epoch 00003: loss improved from 2.89684 to 2.87102, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 2.8340\n",
      "\n",
      "Epoch 00004: loss improved from 2.87102 to 2.83396, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 2.7653\n",
      "\n",
      "Epoch 00005: loss improved from 2.83396 to 2.76535, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 2.6921\n",
      "\n",
      "Epoch 00006: loss improved from 2.76535 to 2.69209, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 2.6147\n",
      "\n",
      "Epoch 00007: loss improved from 2.69209 to 2.61471, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 2.5430\n",
      "\n",
      "Epoch 00008: loss improved from 2.61471 to 2.54298, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 2.4824\n",
      "\n",
      "Epoch 00009: loss improved from 2.54298 to 2.48242, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 2.4331\n",
      "\n",
      "Epoch 00010: loss improved from 2.48242 to 2.43311, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 2.3868\n",
      "\n",
      "Epoch 00011: loss improved from 2.43311 to 2.38681, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 2.3489\n",
      "\n",
      "Epoch 00012: loss improved from 2.38681 to 2.34892, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 2.3204\n",
      "\n",
      "Epoch 00013: loss improved from 2.34892 to 2.32035, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 2.2860\n",
      "\n",
      "Epoch 00014: loss improved from 2.32035 to 2.28597, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 2.2619\n",
      "\n",
      "Epoch 00015: loss improved from 2.28597 to 2.26194, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 2.2361\n",
      "\n",
      "Epoch 00016: loss improved from 2.26194 to 2.23606, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 2.2119\n",
      "\n",
      "Epoch 00017: loss improved from 2.23606 to 2.21190, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 2.1933\n",
      "\n",
      "Epoch 00018: loss improved from 2.21190 to 2.19328, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 2.1694\n",
      "\n",
      "Epoch 00019: loss improved from 2.19328 to 2.16937, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 2.1475\n",
      "\n",
      "Epoch 00020: loss improved from 2.16937 to 2.14753, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 2.1287\n",
      "\n",
      "Epoch 00021: loss improved from 2.14753 to 2.12870, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 2.1094\n",
      "\n",
      "Epoch 00022: loss improved from 2.12870 to 2.10940, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 2.0871\n",
      "\n",
      "Epoch 00023: loss improved from 2.10940 to 2.08710, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 2.0697\n",
      "\n",
      "Epoch 00024: loss improved from 2.08710 to 2.06975, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 2.0476\n",
      "\n",
      "Epoch 00025: loss improved from 2.06975 to 2.04760, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 2.0290\n",
      "\n",
      "Epoch 00026: loss improved from 2.04760 to 2.02898, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 2.0046\n",
      "\n",
      "Epoch 00027: loss improved from 2.02898 to 2.00458, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 1.9871\n",
      "\n",
      "Epoch 00028: loss improved from 2.00458 to 1.98714, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 1.9700\n",
      "\n",
      "Epoch 00029: loss improved from 1.98714 to 1.96997, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 1.9437\n",
      "\n",
      "Epoch 00030: loss improved from 1.96997 to 1.94370, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 1.9256\n",
      "\n",
      "Epoch 00031: loss improved from 1.94370 to 1.92561, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.9030\n",
      "\n",
      "Epoch 00032: loss improved from 1.92561 to 1.90304, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.8826\n",
      "\n",
      "Epoch 00033: loss improved from 1.90304 to 1.88257, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.8630\n",
      "\n",
      "Epoch 00034: loss improved from 1.88257 to 1.86296, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.8357\n",
      "\n",
      "Epoch 00035: loss improved from 1.86296 to 1.83572, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.8211\n",
      "\n",
      "Epoch 00036: loss improved from 1.83572 to 1.82105, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 1.7917\n",
      "\n",
      "Epoch 00037: loss improved from 1.82105 to 1.79165, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.7697\n",
      "\n",
      "Epoch 00038: loss improved from 1.79165 to 1.76972, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 1.7527\n",
      "\n",
      "Epoch 00039: loss improved from 1.76972 to 1.75274, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.7204\n",
      "\n",
      "Epoch 00040: loss improved from 1.75274 to 1.72043, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.7027\n",
      "\n",
      "Epoch 00041: loss improved from 1.72043 to 1.70268, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 1.6732\n",
      "\n",
      "Epoch 00042: loss improved from 1.70268 to 1.67324, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.6547\n",
      "\n",
      "Epoch 00043: loss improved from 1.67324 to 1.65467, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.6289\n",
      "\n",
      "Epoch 00044: loss improved from 1.65467 to 1.62886, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 45/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.6080\n",
      "\n",
      "Epoch 00045: loss improved from 1.62886 to 1.60796, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 1.5757\n",
      "\n",
      "Epoch 00046: loss improved from 1.60796 to 1.57567, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.5507\n",
      "\n",
      "Epoch 00047: loss improved from 1.57567 to 1.55074, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.5325\n",
      "\n",
      "Epoch 00048: loss improved from 1.55074 to 1.53250, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 1.5132\n",
      "\n",
      "Epoch 00049: loss improved from 1.53250 to 1.51324, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 1.4831\n",
      "\n",
      "Epoch 00050: loss improved from 1.51324 to 1.48306, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 1.4517\n",
      "\n",
      "Epoch 00051: loss improved from 1.48306 to 1.45175, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 2s 244us/step - loss: 1.4304\n",
      "\n",
      "Epoch 00052: loss improved from 1.45175 to 1.43043, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 1.4172\n",
      "\n",
      "Epoch 00053: loss improved from 1.43043 to 1.41720, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 1.3826\n",
      "\n",
      "Epoch 00054: loss improved from 1.41720 to 1.38261, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 1.3608\n",
      "\n",
      "Epoch 00055: loss improved from 1.38261 to 1.36084, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 1.3409\n",
      "\n",
      "Epoch 00056: loss improved from 1.36084 to 1.34087, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 1.3237\n",
      "\n",
      "Epoch 00057: loss improved from 1.34087 to 1.32365, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 1.2817\n",
      "\n",
      "Epoch 00058: loss improved from 1.32365 to 1.28170, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 1.2620\n",
      "\n",
      "Epoch 00059: loss improved from 1.28170 to 1.26204, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 1.2321\n",
      "\n",
      "Epoch 00060: loss improved from 1.26204 to 1.23210, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 1.2174\n",
      "\n",
      "Epoch 00061: loss improved from 1.23210 to 1.21740, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 1.2023\n",
      "\n",
      "Epoch 00062: loss improved from 1.21740 to 1.20234, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.1631\n",
      "\n",
      "Epoch 00063: loss improved from 1.20234 to 1.16314, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 1.1374\n",
      "\n",
      "Epoch 00064: loss improved from 1.16314 to 1.13743, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 1.1256\n",
      "\n",
      "Epoch 00065: loss improved from 1.13743 to 1.12561, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 1.1024\n",
      "\n",
      "Epoch 00066: loss improved from 1.12561 to 1.10238, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 233us/step - loss: 1.0859\n",
      "\n",
      "Epoch 00067: loss improved from 1.10238 to 1.08592, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 1.0502\n",
      "\n",
      "Epoch 00068: loss improved from 1.08592 to 1.05016, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 1.0331\n",
      "\n",
      "Epoch 00069: loss improved from 1.05016 to 1.03308, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 1.0087\n",
      "\n",
      "Epoch 00070: loss improved from 1.03308 to 1.00866, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.9905\n",
      "\n",
      "Epoch 00071: loss improved from 1.00866 to 0.99053, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.9762\n",
      "\n",
      "Epoch 00072: loss improved from 0.99053 to 0.97620, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.9542\n",
      "\n",
      "Epoch 00073: loss improved from 0.97620 to 0.95423, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 0.9240\n",
      "\n",
      "Epoch 00074: loss improved from 0.95423 to 0.92395, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 0.9039\n",
      "\n",
      "Epoch 00075: loss improved from 0.92395 to 0.90386, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 0.8978\n",
      "\n",
      "Epoch 00076: loss improved from 0.90386 to 0.89784, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 0.8505\n",
      "\n",
      "Epoch 00077: loss improved from 0.89784 to 0.85046, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 0.8520\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.85046\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 0.8371\n",
      "\n",
      "Epoch 00079: loss improved from 0.85046 to 0.83713, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 0.8139\n",
      "\n",
      "Epoch 00080: loss improved from 0.83713 to 0.81385, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 0.7883\n",
      "\n",
      "Epoch 00081: loss improved from 0.81385 to 0.78830, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 0.7679\n",
      "\n",
      "Epoch 00082: loss improved from 0.78830 to 0.76792, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 0.7508\n",
      "\n",
      "Epoch 00083: loss improved from 0.76792 to 0.75081, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.7228\n",
      "\n",
      "Epoch 00084: loss improved from 0.75081 to 0.72285, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 0.7250\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.72285\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 2s 234us/step - loss: 0.7002\n",
      "\n",
      "Epoch 00086: loss improved from 0.72285 to 0.70023, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.6659\n",
      "\n",
      "Epoch 00087: loss improved from 0.70023 to 0.66587, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 0.6726\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.66587\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 2s 246us/step - loss: 0.6500\n",
      "\n",
      "Epoch 00089: loss improved from 0.66587 to 0.65002, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 2s 247us/step - loss: 0.6085\n",
      "\n",
      "Epoch 00090: loss improved from 0.65002 to 0.60850, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 91/100\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 0.6128\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.60850\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 0.6058\n",
      "\n",
      "Epoch 00092: loss improved from 0.60850 to 0.60583, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.5775\n",
      "\n",
      "Epoch 00093: loss improved from 0.60583 to 0.57754, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 0.5458\n",
      "\n",
      "Epoch 00094: loss improved from 0.57754 to 0.54580, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 0.5426\n",
      "\n",
      "Epoch 00095: loss improved from 0.54580 to 0.54256, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 2s 250us/step - loss: 0.5251\n",
      "\n",
      "Epoch 00096: loss improved from 0.54256 to 0.52513, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 2s 245us/step - loss: 0.5308\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.52513\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 0.5030\n",
      "\n",
      "Epoch 00098: loss improved from 0.52513 to 0.50296, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 0.5077\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.50296\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 2s 236us/step - loss: 0.4672\n",
      "\n",
      "Epoch 00100: loss improved from 0.50296 to 0.46722, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1639783d940>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "callbacks_rnn = Part1_RNN().getcallbacks(checkpointstr=checkpointfile_rnn2,\n",
    "                                         eval_cb=None)\n",
    "\n",
    "# model.load_weights('checkpoints/rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5')\n",
    "\n",
    "model.fit(Xsmall, ysmall, \n",
    "          batch_size=500, \n",
    "          epochs=100,\n",
    "          callbacks = callbacks_rnn,\n",
    "          shuffle = False,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we make a given number of predictions (characters) based on this fitted model?   \n",
    "\n",
    "First we predict the next character after following any chunk of characters in the text of length equal to our chosen window size.  Then we remove the first character in our input sequence and tack our prediction onto the end.  This gives us a slightly changed sequence of inputs that still has length equal to the size of our window.  We then feed in this updated input sequence into the model to predict the another character.  Together then we have two predicted characters following our original input sequence.  Repeating this process N times gives us N predicted characters.\n",
    "\n",
    "In the next Python cell we provide you with a completed function that does just this - it makes predictions when given a) a trained RNN model, b) a subset of (window_size) characters from the text, and c) a number of characters to predict (to follow our input subset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:48:33.672627Z",
     "start_time": "2018-06-08T15:48:33.505156Z"
    }
   },
   "outputs": [],
   "source": [
    "# function that uses trained model to predict a desired number of future characters\n",
    "def predict_next_chars(model,input_chars,num_to_predict):     \n",
    "    # create output\n",
    "    predicted_chars = ''\n",
    "    for i in range(num_to_predict):\n",
    "        # convert this round's predicted characters to numerical input    \n",
    "        x_test = np.zeros((1, window_size, len(chars)))\n",
    "        for t, char in enumerate(input_chars):\n",
    "            x_test[0, t, chars_to_indices[char]] = 1.\n",
    "\n",
    "        # make this round's prediction\n",
    "        test_predict = model.predict(x_test,verbose = 0)[0]\n",
    "\n",
    "        # translate numerical prediction back to characters\n",
    "        r = np.argmax(test_predict)                           # predict class of each test input\n",
    "        d = indices_to_chars[r] \n",
    "\n",
    "        # update predicted_chars and input\n",
    "        predicted_chars+=d\n",
    "        input_chars+=d\n",
    "        input_chars = input_chars[1:]\n",
    "    return predicted_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TODO_6'></a>\n",
    "\n",
    "With your trained model try a few subsets of the complete text as input - note the length of each must be exactly equal to the window size.  For each subset use the function above to predict the next 100 characters that follow each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:49:07.932496Z",
     "start_time": "2018-06-08T15:48:57.333032Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "input chars = \n",
      "n his eyes she eclipses and predominates the whole of her sex. it was not that he felt any emotion a\"\n",
      "\n",
      "predicted chars = \n",
      "ceryou hin ingo s prooo han see. i . a kall a the bunss sf pichine a uesan i whic  or angam mas!rris\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "pses and predominates the whole of her sex. it was not that he felt any emotion akin to love for ire\"\n",
      "\n",
      "predicted chars = \n",
      "mmy mysssericlonen. the look the ther mist is ander and i savere an with hames so rern anlls houred \"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "redominates the whole of her sex. it was not that he felt any emotion akin to love for irene adler. \"\n",
      "\n",
      "predicted chars = \n",
      "i sould nogg por our teren. mese, to the chome mas soon on wissere. the listsed st the chave coursan\"\n",
      "\n",
      "------------------\n",
      "input chars = \n",
      "e of her sex. it was not that he felt any emotion akin to love for irene adler. all emotions, and th\"\n",
      "\n",
      "predicted chars = \n",
      "e peite uut so it he choure in. a s s a d art the rave be anserit indedestereculicl yoy ook the thot\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it\n",
    "# get an appropriately sized chunk of characters from the text\n",
    "start_inds = [1, 20, 30, 50]\n",
    "\n",
    "# load in weights\n",
    "model = build_part2_RNN(windowsize=100, numchars=33)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.load_weights('checkpoints/rnn_part2_CuLSTMm_small_textdata_weights-best.hdf5')\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 100)\n",
    "\n",
    "    # print out input characters\n",
    "    print('------------------')\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks ok, but not great.  Now lets try the same experiment with a larger chunk of the data - with the first 100,000 input/output pairs.  \n",
    "\n",
    "Tuning RNNs for a typical character dataset like the one we will use here is a computationally intensive endeavour and thus timely on a typical CPU.  Using a reasonably sized cloud-based GPU can speed up training by a factor of 10.  Also because of the long training time it is highly recommended that you carefully write the output of each step of your process to file.  This is so that all of your results are saved even if you close the web browser you're working out of, as the processes will continue processing in the background but variables/output in the notebook system will not update when you open it again.\n",
    "\n",
    "In the next cell we show you how to create a text file in Python and record data to it.  This sort of setup can be used to record your final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:49:55.307080Z",
     "start_time": "2018-06-08T15:49:55.128605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is only a test \\nthe value of x is 2\\n'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### A simple way to write output to file\n",
    "f = open('my_test_output.txt', 'w')              # create an output file to write too\n",
    "f.write('this is only a test ' + '\\n')           # print some output text\n",
    "x = 2\n",
    "f.write('the value of x is ' + str(x) + '\\n')    # record a variable value\n",
    "f.close()     \n",
    "\n",
    "# print out the contents of my_test_output.txt\n",
    "f = open('my_test_output.txt', 'r')              # create an output file to write too\n",
    "f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this recording devices we can now more safely perform experiments on larger portions of the text.  In the next cell we will use the first 100,000 input/output pairs to train our RNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fit our model to the dataset, then generate text using the trained model in precisely the same generation method applied before on the small dataset.\n",
    "\n",
    "**Note:** your generated words should be - by and large - more realistic than with the small dataset, but you won't be able to generate perfect English sentences even with this amount of data.  A rule of thumb: your model is working well if you generate sentences that largely contain real English words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T15:50:10.299550Z",
     "start_time": "2018-06-08T15:50:09.390023Z"
    }
   },
   "outputs": [],
   "source": [
    "from my_answers import build_part2_RNN\n",
    "# a small subset of our input/output pairs\n",
    "Xlarge = X[:100000,:,:]\n",
    "ylarge = y[:100000,:]\n",
    "\n",
    "checkpointfile_rnn2 = 'rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5'\n",
    "\n",
    "model = build_part2_RNN(windowsize=100, numchars=33)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "callbacks_rnn = Part2_RNN().getcallbacks(checkpointstr=checkpointfile_rnn2,\n",
    "                                         eval_cb=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T16:16:01.633704Z",
     "start_time": "2018-06-08T15:50:15.529361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100000/100000 [==============================] - 17s 166us/step - loss: 2.8693\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.86927, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 2/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 2.5100\n",
      "\n",
      "Epoch 00002: loss improved from 2.86927 to 2.50999, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 3/100\n",
      "100000/100000 [==============================] - 15s 153us/step - loss: 2.2927\n",
      "\n",
      "Epoch 00003: loss improved from 2.50999 to 2.29269, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 4/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 2.1823\n",
      "\n",
      "Epoch 00004: loss improved from 2.29269 to 2.18228, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 5/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 2.1015\n",
      "\n",
      "Epoch 00005: loss improved from 2.18228 to 2.10154, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 6/100\n",
      "100000/100000 [==============================] - 15s 155us/step - loss: 2.0341\n",
      "\n",
      "Epoch 00006: loss improved from 2.10154 to 2.03411, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 7/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.9792\n",
      "\n",
      "Epoch 00007: loss improved from 2.03411 to 1.97922, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 8/100\n",
      "100000/100000 [==============================] - 16s 155us/step - loss: 1.9312\n",
      "\n",
      "Epoch 00008: loss improved from 1.97922 to 1.93124, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 9/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.8896\n",
      "\n",
      "Epoch 00009: loss improved from 1.93124 to 1.88955, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 10/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.8520\n",
      "\n",
      "Epoch 00010: loss improved from 1.88955 to 1.85200, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 11/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.8182\n",
      "\n",
      "Epoch 00011: loss improved from 1.85200 to 1.81825, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 12/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.7876\n",
      "\n",
      "Epoch 00012: loss improved from 1.81825 to 1.78764, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 13/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.7586\n",
      "\n",
      "Epoch 00013: loss improved from 1.78764 to 1.75856, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 14/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.7293\n",
      "\n",
      "Epoch 00014: loss improved from 1.75856 to 1.72933, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 15/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.7037\n",
      "\n",
      "Epoch 00015: loss improved from 1.72933 to 1.70366, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 16/100\n",
      "100000/100000 [==============================] - 15s 155us/step - loss: 1.6778\n",
      "\n",
      "Epoch 00016: loss improved from 1.70366 to 1.67783, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 17/100\n",
      "100000/100000 [==============================] - 15s 155us/step - loss: 1.6534\n",
      "\n",
      "Epoch 00017: loss improved from 1.67783 to 1.65337, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 18/100\n",
      "100000/100000 [==============================] - 16s 155us/step - loss: 1.6295\n",
      "\n",
      "Epoch 00018: loss improved from 1.65337 to 1.62954, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 19/100\n",
      "100000/100000 [==============================] - 16s 156us/step - loss: 1.6071\n",
      "\n",
      "Epoch 00019: loss improved from 1.62954 to 1.60713, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 20/100\n",
      "100000/100000 [==============================] - 15s 155us/step - loss: 1.5848\n",
      "\n",
      "Epoch 00020: loss improved from 1.60713 to 1.58482, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 21/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.5621\n",
      "\n",
      "Epoch 00021: loss improved from 1.58482 to 1.56208, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 22/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.5415\n",
      "\n",
      "Epoch 00022: loss improved from 1.56208 to 1.54152, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 23/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.5210\n",
      "\n",
      "Epoch 00023: loss improved from 1.54152 to 1.52096, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 24/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.5000\n",
      "\n",
      "Epoch 00024: loss improved from 1.52096 to 1.50001, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 25/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.4793\n",
      "\n",
      "Epoch 00025: loss improved from 1.50001 to 1.47927, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 26/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.4596\n",
      "\n",
      "Epoch 00026: loss improved from 1.47927 to 1.45957, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 27/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.4399\n",
      "\n",
      "Epoch 00027: loss improved from 1.45957 to 1.43986, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 28/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.4192\n",
      "\n",
      "Epoch 00028: loss improved from 1.43986 to 1.41924, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 29/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.4000\n",
      "\n",
      "Epoch 00029: loss improved from 1.41924 to 1.39998, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 30/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.3796\n",
      "\n",
      "Epoch 00030: loss improved from 1.39998 to 1.37961, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 31/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.3609\n",
      "\n",
      "Epoch 00031: loss improved from 1.37961 to 1.36089, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 32/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.3409\n",
      "\n",
      "Epoch 00032: loss improved from 1.36089 to 1.34092, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 33/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.3219\n",
      "\n",
      "Epoch 00033: loss improved from 1.34092 to 1.32191, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 34/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.3016\n",
      "\n",
      "Epoch 00034: loss improved from 1.32191 to 1.30165, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 35/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.2825\n",
      "\n",
      "Epoch 00035: loss improved from 1.30165 to 1.28254, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 36/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.2622\n",
      "\n",
      "Epoch 00036: loss improved from 1.28254 to 1.26217, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 37/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.2438\n",
      "\n",
      "Epoch 00037: loss improved from 1.26217 to 1.24377, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 38/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.2231\n",
      "\n",
      "Epoch 00038: loss improved from 1.24377 to 1.22306, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 39/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.2042\n",
      "\n",
      "Epoch 00039: loss improved from 1.22306 to 1.20417, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 40/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.1863\n",
      "\n",
      "Epoch 00040: loss improved from 1.20417 to 1.18625, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 41/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.1653\n",
      "\n",
      "Epoch 00041: loss improved from 1.18625 to 1.16534, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 42/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.1469\n",
      "\n",
      "Epoch 00042: loss improved from 1.16534 to 1.14690, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 43/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.1279\n",
      "\n",
      "Epoch 00043: loss improved from 1.14690 to 1.12786, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 44/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.1095\n",
      "\n",
      "Epoch 00044: loss improved from 1.12786 to 1.10947, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 45/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.0909\n",
      "\n",
      "Epoch 00045: loss improved from 1.10947 to 1.09095, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 46/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.0711\n",
      "\n",
      "Epoch 00046: loss improved from 1.09095 to 1.07106, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 47/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.0530\n",
      "\n",
      "Epoch 00047: loss improved from 1.07106 to 1.05297, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 48/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.0357\n",
      "\n",
      "Epoch 00048: loss improved from 1.05297 to 1.03574, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 49/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.0170\n",
      "\n",
      "Epoch 00049: loss improved from 1.03574 to 1.01701, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 50/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 1.0009\n",
      "\n",
      "Epoch 00050: loss improved from 1.01701 to 1.00092, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 51/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.9827\n",
      "\n",
      "Epoch 00051: loss improved from 1.00092 to 0.98272, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 52/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.9649\n",
      "\n",
      "Epoch 00052: loss improved from 0.98272 to 0.96486, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 53/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.9511\n",
      "\n",
      "Epoch 00053: loss improved from 0.96486 to 0.95112, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 54/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.9354\n",
      "\n",
      "Epoch 00054: loss improved from 0.95112 to 0.93539, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 55/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.9201\n",
      "\n",
      "Epoch 00055: loss improved from 0.93539 to 0.92005, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 56/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.9045\n",
      "\n",
      "Epoch 00056: loss improved from 0.92005 to 0.90445, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 57/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.8893\n",
      "\n",
      "Epoch 00057: loss improved from 0.90445 to 0.88926, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 58/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.8738\n",
      "\n",
      "Epoch 00058: loss improved from 0.88926 to 0.87381, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 59/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.8608\n",
      "\n",
      "Epoch 00059: loss improved from 0.87381 to 0.86081, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 60/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.8458\n",
      "\n",
      "Epoch 00060: loss improved from 0.86081 to 0.84575, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 61/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.8334\n",
      "\n",
      "Epoch 00061: loss improved from 0.84575 to 0.83336, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 62/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.8179\n",
      "\n",
      "Epoch 00062: loss improved from 0.83336 to 0.81790, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 63/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.8077\n",
      "\n",
      "Epoch 00063: loss improved from 0.81790 to 0.80768, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 64/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7960\n",
      "\n",
      "Epoch 00064: loss improved from 0.80768 to 0.79604, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7828\n",
      "\n",
      "Epoch 00065: loss improved from 0.79604 to 0.78284, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 66/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7709\n",
      "\n",
      "Epoch 00066: loss improved from 0.78284 to 0.77094, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 67/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7612\n",
      "\n",
      "Epoch 00067: loss improved from 0.77094 to 0.76121, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 68/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7494\n",
      "\n",
      "Epoch 00068: loss improved from 0.76121 to 0.74938, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 69/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7367\n",
      "\n",
      "Epoch 00069: loss improved from 0.74938 to 0.73668, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 70/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7279\n",
      "\n",
      "Epoch 00070: loss improved from 0.73668 to 0.72790, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 71/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7182\n",
      "\n",
      "Epoch 00071: loss improved from 0.72790 to 0.71820, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 72/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7091\n",
      "\n",
      "Epoch 00072: loss improved from 0.71820 to 0.70911, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 73/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.7003\n",
      "\n",
      "Epoch 00073: loss improved from 0.70911 to 0.70035, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 74/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6899\n",
      "\n",
      "Epoch 00074: loss improved from 0.70035 to 0.68993, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 75/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6810\n",
      "\n",
      "Epoch 00075: loss improved from 0.68993 to 0.68096, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 76/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6730\n",
      "\n",
      "Epoch 00076: loss improved from 0.68096 to 0.67305, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 77/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6646\n",
      "\n",
      "Epoch 00077: loss improved from 0.67305 to 0.66459, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 78/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6563\n",
      "\n",
      "Epoch 00078: loss improved from 0.66459 to 0.65629, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 79/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6498\n",
      "\n",
      "Epoch 00079: loss improved from 0.65629 to 0.64976, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 80/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6396\n",
      "\n",
      "Epoch 00080: loss improved from 0.64976 to 0.63959, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 81/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6343\n",
      "\n",
      "Epoch 00081: loss improved from 0.63959 to 0.63427, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 82/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6264\n",
      "\n",
      "Epoch 00082: loss improved from 0.63427 to 0.62637, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 83/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6185\n",
      "\n",
      "Epoch 00083: loss improved from 0.62637 to 0.61845, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 84/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6121\n",
      "\n",
      "Epoch 00084: loss improved from 0.61845 to 0.61206, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 85/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.6039\n",
      "\n",
      "Epoch 00085: loss improved from 0.61206 to 0.60388, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 86/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5992\n",
      "\n",
      "Epoch 00086: loss improved from 0.60388 to 0.59918, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 87/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5910\n",
      "\n",
      "Epoch 00087: loss improved from 0.59918 to 0.59098, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 88/100\n",
      "100000/100000 [==============================] - 15s 155us/step - loss: 0.5849\n",
      "\n",
      "Epoch 00088: loss improved from 0.59098 to 0.58495, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 89/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5797\n",
      "\n",
      "Epoch 00089: loss improved from 0.58495 to 0.57967, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 90/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5735\n",
      "\n",
      "Epoch 00090: loss improved from 0.57967 to 0.57349, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 91/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5679\n",
      "\n",
      "Epoch 00091: loss improved from 0.57349 to 0.56791, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 92/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5600\n",
      "\n",
      "Epoch 00092: loss improved from 0.56791 to 0.55998, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 93/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5563\n",
      "\n",
      "Epoch 00093: loss improved from 0.55998 to 0.55635, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 94/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5500\n",
      "\n",
      "Epoch 00094: loss improved from 0.55635 to 0.54997, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 95/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5445\n",
      "\n",
      "Epoch 00095: loss improved from 0.54997 to 0.54452, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 96/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5388\n",
      "\n",
      "Epoch 00096: loss improved from 0.54452 to 0.53877, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 97/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5344\n",
      "\n",
      "Epoch 00097: loss improved from 0.53877 to 0.53442, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 98/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5286\n",
      "\n",
      "Epoch 00098: loss improved from 0.53442 to 0.52863, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 99/100\n",
      "100000/100000 [==============================] - 15s 154us/step - loss: 0.5256\n",
      "\n",
      "Epoch 00099: loss improved from 0.52863 to 0.52561, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n",
      "Epoch 100/100\n",
      "100000/100000 [==============================] - 15s 155us/step - loss: 0.5162\n",
      "\n",
      "Epoch 00100: loss improved from 0.52561 to 0.51623, saving model to F:\\git\\aind\\deeplearn\\aind2-rnn\\checkpoints\\rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x163992c1438>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_weights('checkpoints/rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5')\n",
    "\n",
    "model.fit(Xlarge, ylarge, \n",
    "          batch_size=1000, \n",
    "          epochs=100,\n",
    "          callbacks = callbacks_rnn,\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-08T16:22:25.608902Z",
     "start_time": "2018-06-08T16:22:16.962237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "\n",
      "input chars = \n",
      "akin to love for irene adler. all emotions, and that one particularly, were abhorrent to his cold, p\"\n",
      "\n",
      "predicted chars = \n",
      "rom holmes. i shall not she wime he room his byeart, peeshing he ease ever then if you me and from i\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      "recise but admirably balanced mind. he was, i take it, the most perfect reasoning and observing mach\"\n",
      "\n",
      "predicted chars = \n",
      " sparied to me, showing might he he entered, the same to my be seell to door and i have his noters o\"\n",
      "\n",
      "-------------------\n",
      "\n",
      "input chars = \n",
      "server excellent for drawing the veil from men s motives and actions. but for the trained reasoner t\"\n",
      "\n",
      "predicted chars = \n",
      "o the ross that it mockst who had come tagked at the rusmien door, and his brey upbly the bass, this\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: choose an input sequence and use the prediction function in the previous Python cell to predict 100 characters following it\n",
    "\n",
    "# get an appropriately sized chunk of characters from the text\n",
    "start_inds = [100,200,500]\n",
    "\n",
    "# load in weights\n",
    "model = build_part2_RNN(windowsize=100, numchars=33)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizer)\n",
    "model.load_weights('checkpoints/rnn_part2_CuLSTMm_LARGE_textdata_weights-best.hdf5')\n",
    "\n",
    "# save output\n",
    "f = open('text_gen_output/RNN_large_textdata_output.txt', 'w')  # create an output file to write too\n",
    "\n",
    "for s in start_inds:\n",
    "    start_index = s\n",
    "    input_chars = text[start_index: start_index + window_size]\n",
    "\n",
    "    # use the prediction function\n",
    "    predict_input = predict_next_chars(model,input_chars,num_to_predict = 100)\n",
    "\n",
    "    # print out input characters\n",
    "    line = '-------------------' + '\\n'\n",
    "    print(line)\n",
    "    f.write(line)\n",
    "\n",
    "    input_line = 'input chars = ' + '\\n' +  input_chars + '\"' + '\\n'\n",
    "    print(input_line)\n",
    "    f.write(input_line)\n",
    "\n",
    "    # print out predicted characters\n",
    "    predict_line = 'predicted chars = ' + '\\n' +  predict_input + '\"' + '\\n'\n",
    "    print(predict_line)\n",
    "    f.write(predict_line)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 758,
   "position": {
    "height": "40px",
    "left": "456px",
    "right": "20px",
    "top": "64px",
    "width": "526px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
